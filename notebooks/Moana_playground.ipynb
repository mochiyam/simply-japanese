{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315f2705-9f10-4e2e-84a3-40179a9c0434",
   "metadata": {},
   "source": [
    "# Installations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03455e-5133-4a15-b83c-8549dfcb8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029a90-a95f-456e-a85a-d48b4e21e703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#These wheels include a copy of the MeCab library, but not a dictionary. \n",
    "#In order to use MeCab you'll need to install a dictionary. unidic-lite is a good one to start with:\n",
    "# !pip install unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61c655-fca4-4ca7-8284-aeb4a1832012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization tool\n",
    "# !pip install neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98247d2a-0bde-4094-9746-31b824c9cc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adeeaf-d7ee-4bce-b8dc-720ca4e04d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to see Japanese!\n",
    "# !pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f62505-0939-4237-ab1d-c8910f324506",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca6f550-d324-4e2c-b3f5-519c2b87dd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "import MeCab\n",
    "import neologdn\n",
    "import collections\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "#import seaborn as sns # REMINDER: make sure to remove if not using!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ab98f-2432-4694-bf22-c671dd7bc913",
   "metadata": {},
   "source": [
    "# Just having fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89616cf-a1e9-45ab-8389-9a9dd056ddca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#日本語(原文)</th>\n",
       "      <th>#やさしい日本語</th>\n",
       "      <th>#英語(原文)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>月曜日までにこの仕事を終えて下さい。</td>\n",
       "      <td>月曜日までにこの仕事を終わらせてください。</td>\n",
       "      <td>please get this work finished by monday .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>失敗してもあきらめてはいけない。</td>\n",
       "      <td>失敗してもダメと思ってはならない。</td>\n",
       "      <td>don 't give up if you fail .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>あなたは何を見つめているのですか。</td>\n",
       "      <td>あなたは何を見ているのですか。</td>\n",
       "      <td>what are you gazing at ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>その女の子は母と似ていた。</td>\n",
       "      <td>その少女は母と似ていた。</td>\n",
       "      <td>the girl resembled her mother .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彼は貧しかったので、大学へ行けなかった。</td>\n",
       "      <td>彼はお金がなかったので、大学へ行くことができなかった。</td>\n",
       "      <td>poor as he was , couldn 't go to college .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               #日本語(原文)                     #やさしい日本語  \\\n",
       "0    月曜日までにこの仕事を終えて下さい。        月曜日までにこの仕事を終わらせてください。   \n",
       "1      失敗してもあきらめてはいけない。            失敗してもダメと思ってはならない。   \n",
       "2     あなたは何を見つめているのですか。              あなたは何を見ているのですか。   \n",
       "3         その女の子は母と似ていた。                 その少女は母と似ていた。   \n",
       "4  彼は貧しかったので、大学へ行けなかった。  彼はお金がなかったので、大学へ行くことができなかった。   \n",
       "\n",
       "                                      #英語(原文)  \n",
       "0   please get this work finished by monday .  \n",
       "1                don 't give up if you fail .  \n",
       "2                    what are you gazing at ?  \n",
       "3             the girl resembled her mother .  \n",
       "4  poor as he was , couldn 't go to college .  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_150.xlsx'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee66411f-b4db-4785-853f-0cfc309b26d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#日本語(原文)</th>\n",
       "      <th>#やさしい日本語</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>月曜日までにこの仕事を終えて下さい。</td>\n",
       "      <td>月曜日までにこの仕事を終わらせてください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>失敗してもあきらめてはいけない。</td>\n",
       "      <td>失敗してもダメと思ってはならない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>あなたは何を見つめているのですか。</td>\n",
       "      <td>あなたは何を見ているのですか。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>その女の子は母と似ていた。</td>\n",
       "      <td>その少女は母と似ていた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彼は貧しかったので、大学へ行けなかった。</td>\n",
       "      <td>彼はお金がなかったので、大学へ行くことができなかった。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               #日本語(原文)                     #やさしい日本語\n",
       "0    月曜日までにこの仕事を終えて下さい。        月曜日までにこの仕事を終わらせてください。\n",
       "1      失敗してもあきらめてはいけない。            失敗してもダメと思ってはならない。\n",
       "2     あなたは何を見つめているのですか。              あなたは何を見ているのですか。\n",
       "3         その女の子は母と似ていた。                 その少女は母と似ていた。\n",
       "4  彼は貧しかったので、大学へ行けなかった。  彼はお金がなかったので、大学へ行くことができなかった。"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ccc51b-a9ef-4b91-b7a7-d75fc04d9436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>月曜日までにこの仕事を終えて下さい。</td>\n",
       "      <td>月曜日までにこの仕事を終わらせてください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>失敗してもあきらめてはいけない。</td>\n",
       "      <td>失敗してもダメと思ってはならない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>あなたは何を見つめているのですか。</td>\n",
       "      <td>あなたは何を見ているのですか。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>その女の子は母と似ていた。</td>\n",
       "      <td>その少女は母と似ていた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彼は貧しかったので、大学へ行けなかった。</td>\n",
       "      <td>彼はお金がなかったので、大学へ行くことができなかった。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original                   simplified\n",
       "0    月曜日までにこの仕事を終えて下さい。        月曜日までにこの仕事を終わらせてください。\n",
       "1      失敗してもあきらめてはいけない。            失敗してもダメと思ってはならない。\n",
       "2     あなたは何を見つめているのですか。              あなたは何を見ているのですか。\n",
       "3         その女の子は母と似ていた。                 その少女は母と似ていた。\n",
       "4  彼は貧しかったので、大学へ行けなかった。  彼はお金がなかったので、大学へ行くことができなかった。"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96de15-7c9d-4f6d-ba5c-7a3c343fca53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "text = df['original'][0]\n",
    "parsed = tagger.parse(text)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e0d53-c1fd-460e-b5be-f6431ccb3f96",
   "metadata": {},
   "source": [
    "名詞 - noun\n",
    "助詞 - particle\n",
    "連体詞 - \n",
    "動詞\n",
    "補助記号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b5832-6f89-4944-a2f4-73dda1c596ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just testing stuff out\n",
    "test = MeCab.Tagger(\"-O wakati\") \n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "parsed = test.parse(text)\n",
    "print(parsed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898134a-c7ed-4bc1-accd-82952939bf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db857a77-0bef-4b8a-bcc2-ce3fb8cb4c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = MeCab.Tagger(\"r'-d /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages'\")\n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "parsed = test.parse(text)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f5a7d-39bb-4459-9d88-cd262d623da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Super dumb dumb method\n",
    "def count_all_word_frequency():\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger()\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['original']\n",
    "        node = t.parseToNode(text)\n",
    "        while node:\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "all_words = count_all_word_frequency()\n",
    "# tuples in a list\n",
    "print(all_words.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a106d0-2ddd-413c-b6eb-8c2987ed3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756c156-9908-438d-b7e6-4c25f434fde0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_word_frequency(word_freq, most_common_num):\n",
    "    freq_dist = FreqDist(word_freq)\n",
    "    freq_dist.plot(most_common_num,cumulative=False)\n",
    "#plot_word_frequency(all_words, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e25be-c729-4ba7-8c7c-9d5d416cf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super dumb dumb method\n",
    "def count_all_word_frequency():\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger()\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['original']\n",
    "        node = t.parseToNode(text)\n",
    "        while node:\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "all_words = count_all_word_frequency()\n",
    "# tuples in a list\n",
    "print(all_words.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d496b27-2583-4485-81e2-d81045a648bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = MeCab.Tagger(\"-O wakati\")\n",
    "print(text)\n",
    "text = \"あなたは何を見つめているのですか。\"\n",
    "parsed = test.parse(text)\n",
    "node = test.parseToNode(text).next\n",
    "while node.next:\n",
    "    print(node.surface, node.feature.split(',')[0])\n",
    "    node = node.next\n",
    "#node.surface.decode(\"utf-8\", \"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b65bc2-121d-4ed8-9fb5-85a9c0dc7930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#               助詞           \n",
    "#              /\n",
    "# Remove 付属語 \n",
    "#　　　　　　　 \\\n",
    "#             　 助動詞\n",
    "\n",
    "#月 が｜きれいな｜晩 でし た 。\n",
    "#付属語 : が　・　でした"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15707b5b-6e9b-4a77-b5b9-53742c43bab7",
   "metadata": {},
   "source": [
    "# With 10_000 Data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa144e1-fee7-4e81-948c-208ddf12a95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_10000.xlsx'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14a209-db7c-4e8a-9731-52b6ca1e2385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fe375-71f2-4dd6-b289-843017d49610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918856f2-9d73-4bff-9290-4f44c2cd369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counts all the independent word 自立語\n",
    "_stopwords = stopwords.words('japanese')\n",
    "\n",
    "def count_all_words(docs, col='original'):\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in docs.iterrows():\n",
    "        text = row[col]\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            part_of_speech = node.feature.split(',')[0]\n",
    "            # REPLACE_WORD_POS = (\"名詞\", \"動詞\", \"形容詞\", \"副詞\", \"未知語\") # TBD\n",
    "            # IGNORE = (\"接尾\", \"非自立\", \"代名詞\")    \n",
    "            if part_of_speech in [\"助動詞\", \"助詞\", \"補助記号\"] or node.surface in _stopwords:\n",
    "                node = node.next\n",
    "                continue\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "ind_word_freq = count_all_words(df)\n",
    "plot_word_frequency(ind_word_freq, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50c4db-7fbc-41d8-8570-61eee145858d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_2000_word_freq = ind_word_freq.most_common(2000)\n",
    "top_2000_word_freq[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe1af4-2a80-49c9-a13b-af3b1d3bb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find sentences that are exactly the same \n",
    "# 2. temp_list of tokens for sentence original and simplified\n",
    "# 3. Compare the two temp_list\n",
    "# 4. two global_lists of deleted and added(simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fc1f9-1bf6-48bb-a71c-e803a8096d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp = df.head(10)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b78a53-173a-4579-9245-07a85336c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1.  Get the corpuses that are different from original and simplified \n",
    "diff_corpus_df = df[df['original'] != df['simplified']]\n",
    "diff_corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa90c26-f8b4-4d25-8681-952e90af9d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Create a temp_list of tokens for sentence original and simplified\n",
    "original_temp_list = count_all_words(diff_corpus_df, 'original')\n",
    "simplified_temp_list = count_all_words(diff_corpus_df, 'simplified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67456dc-5cdf-400f-a019-6baa95bf35d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simplified_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622171e-99a0-45fd-8954-be66c9081ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e5072-9be0-4bd4-91d5-01b3307be918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dict(original_temp_list).items(), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0b23b-a639-4c98-b3df-d2148d8c2e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Compare the two temp_list\n",
    "\n",
    "# Collections library\n",
    "# Elements are subtracted from an iterable or from another mapping (or counter). \n",
    "# Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative.\n",
    "diff_temp_df = simplified_temp_list\n",
    "diff_temp_df.subtract(original_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be54158-304a-4ddc-ac26-249acab854f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_temp_df[diff_temp_df['count'] < 0].sort_values(by='count').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5059e4-b979-4ebf-bbbf-9fa66e970a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. two global_lists of deleted and added(simplified)\n",
    "deleted = []\n",
    "added = []\n",
    "\n",
    "diff_temp_df = pd.DataFrame(dict(diff_temp).items(), columns=['word', 'count'])\n",
    "deleted =  diff_temp_df[diff_temp_df['count'] < 0]['word'].tolist()\n",
    "added = diff_temp_df[diff_temp_df['count'] >= 0]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e38ab0-0745-4a40-94da-cc5e37065a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5dae2-6d5f-4fe4-a65d-67521ed09333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(deleted), len(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585661b7-72de-40e7-9a8c-d7eec4684138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f4a9a-2f5d-4955-9e9f-bf0c1ebad0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45642c23-3bda-4cb1-b067-472e7ce4abc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f745acf-cb82-43c6-9060-043752f5706a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827197b-1c22-4a9e-a963-602656648cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bda8ed35-eb62-47b3-a6ed-fe3837290ddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909cc3b-3366-479f-b452-ede5bfc20c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_1000.xlsx'))\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7e921-f425-4959-b8d7-055cdba75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LENGTH = len(df)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = DATA_LENGTH\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = DATA_LENGTH\n",
    "EMBEDDING_SIZE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107de50-777d-4577-a3a7-12effb8905d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq : Encoder LSTM -Decoder LSTM architecture\n",
    "\n",
    "original_sentence = df['original'].to_list()\n",
    "sos_simplified_sentence = [f'<sos> {sentence}' for sentence in df['simplified'].to_list()]\n",
    "eof_simplified_sentence = df['simplified'].str.cat(['<eof>' for _ in range(DATA_LENGTH)], sep =' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bf2d1-c297-41c1-a2c9-903b9884aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebac0f-84f7-4ccd-87f3-bcc2365d3a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048755c-2432-4782-a842-cf1f2fb8fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e1345-1743-4cfa-a757-8a1fa3ecf5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e181a05-60ce-41fa-a715-41147a9a6b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4200a01-810c-4f68-b2de-8dea95ba7be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c174087-2120-488e-92ba-1ad4a5768193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# please work\n",
    "model = word2vec.Word2Vec.load('word2vec.gensim.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58775d20-3c56-42b3-84ef-9c5302ed46b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.wv['なまえ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c966ec-e4f5-4c4e-a791-bc90c0037071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('ただいま', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902540bb-3da3-4a70-b9b2-8aa15c15028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v1 = model.wv['ただいま']\n",
    "v2 = model.wv['本日']\n",
    "res = v1 - v2\n",
    "model.wv.similarity('いま', '今')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61356774-7467-45a4-bf46-4686323d085f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082274f-4fa4-4b6b-a545-90719aa22534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list = diff_temp_df['word'].to_list()\n",
    "w2v = word2vec.Word2Vec(list, vector_size=10,\n",
    "                        window=5,\n",
    "                        min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dae01c-d78f-42ea-b68e-ace8ff709f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# w2v.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd897de-3ae6-468d-a6cd-b262af942d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b271102-2eba-48db-a353-2e0122b73a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#代名詞、名詞、動詞\n",
    "\n",
    "test = MeCab.Tagger()\n",
    "text = \"ただいま話し中です。\"\n",
    "parsed = test.parse(text)\n",
    "node = test.parseToNode(text).next\n",
    "while node.next:\n",
    "    print(node.surface, node.feature)\n",
    "    node = node.next\n",
    "#node.surface.decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6719911-26f4-4469-b1f8-93a559d06215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369a775-5d17-4789-b2fc-af9f01fa02a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb83c4-ffad-4c13-8705-ce8855704e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ac85930-6f8a-4efd-876f-e8d491b9e7fc",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder Transformation Model... attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62550af8-f4ea-4848-acdf-ea38d9d085e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fe2df-1e1d-41ab-92ff-8484ca234157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_sentences = df['original'].to_list()\n",
    "simplified_sentences = df['simplified'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a642347-34cd-4061-8ef9-78de284ac25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.gensim.model\")\n",
    "# len(words) = 335477\n",
    "words = [\"<PAD>\"] + model.wv.index_to_key\n",
    "# embedding.shape = (335477, 50)\n",
    "embedding = np.insert(model.wv.vectors, 0, 0, axis=0)\n",
    "# Dictionary of word and its index\n",
    "input_token = target_token = dict((w, i) for i, w in enumerate(words))\n",
    "encoder_tokens = decoder_tokens = embedding.shape[0] # for Masking > 335477\n",
    "max_encoder_seq_length = max(len(sentence) for sentence in original_sentence) # > 28\n",
    "\n",
    "# Input and output vocabulary sizes (types of words)\n",
    "# Prepend BOS (Beginning Of Sentence) at the beginning of the sentence  \n",
    "max_decoder_seq_length = max_encoder_seq_length + 1 # <BOS> 29\n",
    "output_dim = embedding.shape[1] # > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ece98-1509-476a-910b-0416de628e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip list | grep tensorflow\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff420e40-1eaf-4567-9615-f851ef0b31fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Activation, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9f184-5dd8-44a8-8947-6228dce81d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8e52d-9550-4fa6-8e33-cffd1295b864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_dimension = 64\n",
    "\n",
    "# Embedding\n",
    "layer_emb = Embedding(input_dim=encoder_tokens,\n",
    "                      output_dim=output_dim,\n",
    "                      trainable=False,\n",
    "                      mask_zero=True)\n",
    "# Encoder\n",
    "# Input() is used to instantiate a Keras tensor\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "x = layer_emb(encoder_inputs)\n",
    "# Takes the hidden state and internal state of this Embedding layer\n",
    "# state_h : hidden state in a cell, state_c : memory cell internal state\n",
    "_, state_h, state_c = LSTM(hidden_dimension, return_sequences=True, return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49d9d9-b963-4834-8ff0-c96a2caa65e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "x = layer_emb(decoder_inputs)\n",
    "x, _, _ = LSTM(hidden_dimension, return_sequences=True, return_state=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(decoder_tokens)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc395950-0c70-45a7-bf09-b78f1e5ece0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_masking(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(tf.gather_nd(y_true, tf.where(y_pred._keras_mask)), tf.gather_nd(y_pred, tf.where(y_pred._keras_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7a465-2055-41bc-8f27-31fb71fc07be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = RMSprop(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=lambda y_true, y_pred: tf.nn.softmax_cross_entropy_with_logits(tf.one_hot(tf.cast(y_true, tf.int32), num_decoder_tokens), y_pred),\n",
    "              metrics=[accuracy_masking])\n",
    "# set embedding matrix\n",
    "layer_emb.set_weights([embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d52cd-bf44-4730-99b3-d8705aa174b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb87c1-064f-48a8-be02-40af8c6eac30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(original_sentences), max_encoder_seq_length))\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(original_sentences), max_decoder_seq_length))\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(original_sentences), max_decoder_seq_length))\n",
    "\n",
    "len(encoder_input_data), len(decoder_input_data), len(decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06df847-3f38-4722-b38e-4024b4caa0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_token['<BOS>'] = len(input_token)\n",
    "input_token['<EOS>'] = len(input_token) + 1\n",
    "input_token['<BOS>'], input_token['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a878a2a-7f27-4706-8928-1d38261f8827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_token['<BOS>'] = len(target_token)\n",
    "target_token['<EOS>'] = len(target_token) + 1\n",
    "target_token['<BOS>'], target_token['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76284ae-5b52-4976-9f32-e38d613481b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (original_sentence, simplified_sentence) in enumerate(zip(original_sentences, simplified_sentences)):\n",
    "    for t, w in enumerate(original_sentence):\n",
    "        encoder_input_data[i, t] = input_token[w]\n",
    "\n",
    "    decoder_input_data[i, 0] = target_token['<BOS>'] # BOS\n",
    "    for t, w in enumerate(simplified_sentence):\n",
    "        decoder_input_data[i, t + 1] = target_token[w]\n",
    "        decoder_target_data[i, t] = target_token[w]\n",
    "    decoder_target_data[i, t + 1:] = target_token['<EOS>'] # EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd82a2-c4cd-4145-93b4-dcaa4dda0908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f3b3d-7b88-4002-8d37-313e976734b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2790892-413b-489f-8f98-534de4c8d222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bebcf-4d76-4ba7-9b47-42af9cba5713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c774b-b21d-4568-859d-ea3639a4e50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ddd8-8994-41a6-8560-1155f343a161",
   "metadata": {},
   "source": [
    "### Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d40f96-03c7-480e-ab62-68fdf9a7145a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Process the dataset\n",
    "Append SOS and EOS\n",
    "\"\"\"\n",
    "\n",
    "df[\"simplified_w_marker\"] = [f'<sos> {sentence} <eof>' for sentence in df['simplified']]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652c259-0e6e-45f9-8601-66618355c660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab(df, col):\n",
    "    vocabulary = []\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['simplified_w_marker']\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            vocabulary.append(node.surface)\n",
    "            node = node.next\n",
    "    vocabulary = sorted(set(vocabulary)) + ['<unk>']\n",
    "    word2idx = dict((idx, vocab) for idx, vocab in enumerate(vocabulary))\n",
    "    idx2word = dict((vocab, idx) for idx, vocab in enumerate(vocabulary))\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95666cb7-18fe-4d4d-af6f-5260e6889c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40965d0a-8d36-4120-a96e-6cbb998775ab",
   "metadata": {},
   "source": [
    "### Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4016300-bec4-41bb-9042-884b537d55ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X : original sentence\n",
    "# y : simplified sentence\n",
    "X_word2idx, X_idx2word = get_vocab(df, 'original')\n",
    "y_word2idx, y_idx2word = get_vocab(df, 'simpflied_w_marker')\n",
    "\n",
    "X_train = df['original'].to_list()\n",
    "y_train = df[\"simplified_w_marker\"].to_list()\n",
    "\n",
    "X_vocab_size = len(X_word2idx)\n",
    "y_vocab_size = len(y_word2idx)\n",
    "\n",
    "hidden_dimension = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90c8b4-f0d1-4526-95b6-3eb467093ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "layer_emb = Embedding(input_dim=encoder_tokens,\n",
    "                      output_dim=output_dim,\n",
    "                      trainable=False,\n",
    "                      mask_zero=True)\n",
    "# Encoder\n",
    "# Input() is used to instantiate a Keras tensor\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "# x = layer_emb(encoder_inputs)\n",
    "\n",
    "# Takes the hidden state and internal state of this Embedding layer\n",
    "# state_h : hidden state in a cell, state_c : memory cell internal state\n",
    "encoder_lstm = LSTM(hidden_dimension, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "# x = layer_emb(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_dimension, return_sequences=True, return_state=True)\n",
    "decoder_dense_layer = Dense(y_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb91a3-51e3-48fd-b143-a89287abc084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(original_sentences)\n",
    "y = np.array(simplified_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377b1b5-f9d7-4660-948d-6f18107b8ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(df)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(df),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X[is_train], y[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X[~is_train], y[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfc39c-2141-4b28-bc0f-229f344a3a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.strings.unicode_decode(X, 'UTF-8').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05e6d8-d896-4885-a7d4-ac4f8313d52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206b388-5d8b-43bc-aaf3-b79e6ea679ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9ad37-0b3b-44d9-86ec-1d990cbd4de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow_text as tf_text\n",
    "example_text = tf.constant('父は私が外国へ行くことを承知した。')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7970f-8d29-40ce-befb-1c61f1a092a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81f330-b93a-4ce4-a37b-e035b06ef8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46d0ed-fdab-4acb-8da8-0a8bd5e7a211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08c747-4018-450c-94dc-4b327cd19eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b79cd-9366-48ab-9fce-7bc7052b9a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c65a0-39e8-48e2-a096-d694c364a75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb9820-428f-44b7-8d98-e38b304a7530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06f819-8ccf-4d46-906c-5063104e346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers==0.10.3\n",
    "# !pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bea09f-1055-45a8-8473-b515740ac9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sonoisa/t5-base-japanese\")\n",
    "model = AutoModel.from_pretrained(\"sonoisa/t5-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91666b-5713-45aa-9f02-4f8576a7efb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09cf88-c81a-4930-a93b-edab65d12459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcea2c-b388-4215-9f08-02a0bbcc2e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4e79b-11c0-4986-9ec1-d0114954f15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b010615-32c8-409b-a113-5e191c6ff5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07cfb38f-c16c-4a01-a389-25dcdc6a222f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>月曜日までにこの仕事を終えて下さい。</td>\n",
       "      <td>月曜日までにこの仕事を終わらせてください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>失敗してもあきらめてはいけない。</td>\n",
       "      <td>失敗してもダメと思ってはならない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>あなたは何を見つめているのですか。</td>\n",
       "      <td>あなたは何を見ているのですか。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>その女の子は母と似ていた。</td>\n",
       "      <td>その少女は母と似ていた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彼は貧しかったので、大学へ行けなかった。</td>\n",
       "      <td>彼はお金がなかったので、大学へ行くことができなかった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>彼女はことわざをいくつも知っている。</td>\n",
       "      <td>彼女は昔から知られてい短い言葉をいくつも知っている。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>私たちのチームが勝つことを確信しています。</td>\n",
       "      <td>私たちのチームが勝つと思っている。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>私は日本の歴史に興味がある。</td>\n",
       "      <td>私は日本の歴史に興味がある。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>私は時折学校で彼女に会う。</td>\n",
       "      <td>私は時々学校で彼女に会う。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>電車はどこで乗れますか。</td>\n",
       "      <td>列車はどこで乗ることができますか。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  original                   simplified\n",
       "0       月曜日までにこの仕事を終えて下さい。        月曜日までにこの仕事を終わらせてください。\n",
       "1         失敗してもあきらめてはいけない。            失敗してもダメと思ってはならない。\n",
       "2        あなたは何を見つめているのですか。              あなたは何を見ているのですか。\n",
       "3            その女の子は母と似ていた。                 その少女は母と似ていた。\n",
       "4     彼は貧しかったので、大学へ行けなかった。  彼はお金がなかったので、大学へ行くことができなかった。\n",
       "..                     ...                          ...\n",
       "145     彼女はことわざをいくつも知っている。   彼女は昔から知られてい短い言葉をいくつも知っている。\n",
       "146  私たちのチームが勝つことを確信しています。            私たちのチームが勝つと思っている。\n",
       "147         私は日本の歴史に興味がある。               私は日本の歴史に興味がある。\n",
       "148          私は時折学校で彼女に会う。                私は時々学校で彼女に会う。\n",
       "149           電車はどこで乗れますか。            列車はどこで乗ることができますか。\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05eb8284-d75c-41e9-bea4-17cbda6b3aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['original']]\n",
    "y = df[['simplified']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee561b29-91cd-4a7b-8e9e-d7655b01995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee2a736-50b5-4f93-ae86-d7f5a712c9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-21 02:45:36.310793: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 02:45:36.634563: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-21 02:45:36.690978: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 02:45:36.690998: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 02:45:38.002782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 02:45:38.002893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 02:45:38.002901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.t5 import T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8410281c-dbf4-4758-a72e-c8ac24474692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class Cfg:\n",
    "    seed = 42\n",
    "    cuda = torch.cuda.is_available()\n",
    "    prefix = 'easy_japanese'\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "029f69cd-b666-4ec9-ba5d-c884b28641f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████| 553/553 [00:00<00:00, 126kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|█████████████████████████████████████████| 1.20G/1.20G [00:56<00:00, 21.4MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████████████████████████████████████████████| 147/147 [00:00<00:00, 60.7kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|█████████████████████████████████████████| 4.31M/4.31M [00:02<00:00, 1.71MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████| 99.0/99.0 [00:00<00:00, 27.4kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████| 82.0/82.0 [00:00<00:00, 34.9kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "evaluate_during_training is enabled but eval_data is not specified. Pass eval_data to model.train_model() if using evaluate_during_training.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_accumulation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepetition_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# 'dataloader_num_workers': multiprocessing.cpu_count()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m }\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m T5Model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmt5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m=\u001b[39mtrain_params, use_cuda\u001b[38;5;241m=\u001b[39mCfg\u001b[38;5;241m.\u001b[39mcuda)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/simpletransformers/t5/t5_model.py:205\u001b[0m, in \u001b[0;36mT5Model.train_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# if self.args.silent:\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m#     show_running_loss = False\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mevaluate_during_training \u001b[38;5;129;01mand\u001b[39;00m eval_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluate_during_training is enabled but eval_data is not specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Pass eval_data to model.train_model() if using evaluate_during_training.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_dir:\n\u001b[1;32m    211\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir\n",
      "\u001b[0;31mValueError\u001b[0m: evaluate_during_training is enabled but eval_data is not specified. Pass eval_data to model.train_model() if using evaluate_during_training."
     ]
    }
   ],
   "source": [
    "\n",
    "train_params = {\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'repetition_penalty': 1.5,\n",
    "    'learning_rate': 1e-4, \n",
    "    'max_seq_length': 64, \n",
    "    'max_length': 64, \n",
    "    'train_batch_size': 32, \n",
    "    'eval_batch_size': 32, \n",
    "    'num_train_epochs': 2,\n",
    "    'evaluate_during_training': True,\n",
    "    'evaluate_during_training_steps': 10000,\n",
    "    'use_multiprocessing': False,\n",
    "    'fp16': False,\n",
    "    'save_steps': -1,\n",
    "    'save_eval_checkpoints': False,\n",
    "    'save_model_every_epoch': False,\n",
    "    'no_cache': True,F\n",
    "    'overwrite_output_dir': True,\n",
    "    'preprocess_inputs': False,\n",
    "    'early_stopping_consider_epochs': True, \n",
    "    'output_dir': 'stage1_outputs/',\n",
    "    'best_model_dir': 'stage1_outputs/best_model'\n",
    "    # 'dataloader_num_workers': multiprocessing.cpu_count()\n",
    "}\n",
    "\n",
    "model = T5Model('mt5', 'google/mt5-small', args=train_params, use_cuda=Cfg.cuda)\n",
    "model.train_model(X_train, eval_data=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b9e26-3f67-4246-afe3-14170740dd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cde83e-9707-496d-9186-93a59c4351a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e33e5-c940-452b-8061-5bad19279c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ff1fb3-6530-4341-bafc-a55d2a9eca10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-23 23:52:06.835206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 23:52:07.169445: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-23 23:52:07.237381: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-23 23:52:07.237402: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-23 23:52:08.621410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 23:52:08.621498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 23:52:08.621506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bddfa017-9092-4e90-bd86-ecbfb2d480f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_1000.xlsx'))\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf0c9c1-9b62-40e1-b9fd-4cc110e20c31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"sonoisa/t5-base-japanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2389a7f5-ef8a-4727-8052-200acd590b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original', 'simplified'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['original', 'simplified'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['original', 'simplified'],\n",
       "        num_rows: 875\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "train_ds = datasets.Dataset.from_dict(df.iloc[:100])\n",
    "\n",
    "validation_ds = datasets.Dataset.from_dict(df.iloc[100:125])\n",
    "\n",
    "test_ds = datasets.Dataset.from_dict(df.iloc[125:])\n",
    "\n",
    "raw_datasets = datasets.DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": validation_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38eece59-2503-4a66-955d-0e9b009f032f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'もう夕食は済みましたか。', 'simplified': 'もう夕食は食べ終わりましたか。'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ddfc57-9f94-4818-a04d-6c826fbe15b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745b8952-3a71-49be-9c5c-e6037d03353a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私たちにまだたくさんの仕事がある。</td>\n",
       "      <td>私たちにまだ多くの仕事がある。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>彼を重視しているのですか。</td>\n",
       "      <td>彼を重要だと見ているのですか。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>彼は英語とフランス語をしゃべることができます。</td>\n",
       "      <td>彼は英語とフランス語を話すことができます。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>彼が何をするかわからない。</td>\n",
       "      <td>彼が何をするかわからない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ご家族のみなさんはうまくいっていますか。</td>\n",
       "      <td>ご家族のみんなはうまくいっていますか。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774dc1a6-931e-46f7-bf8d-75f65457b01f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from rouge_score) (1.23.5)\n",
      "Requirement already satisfied: nltk in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: click in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->rouge_score) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->rouge_score) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eff342d-1650-4be1-a29a-0b6b4cdbdef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5044/3153957617.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"rouge\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = datasets.load_metric(\"rouge\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9892fa0-6c42-4517-8913-1572789b104c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 01:46:17.354043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 01:46:17.672286: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-24 01:46:17.725711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 01:46:17.725732: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-24 01:46:19.173158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 01:46:19.173320: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 01:46:19.173328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9451fb6-b658-4e20-ae3b-8f4a0e2afbf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5, 25739, 7, 253, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"こんにちは!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0172e081-80d8-4e7e-b354-6d1b11d75b66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[5, 2, 2068, 2341, 81, 5, 15945, 6885, 1172, 2146, 5092, 10566, 253, 1], [5, 2, 23982, 6885, 1172, 12027, 4648, 2146, 5092, 10566, 67, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this is a sentence!\", \"Thisｂ is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8128e53f-01a0-4f81-b098-686404b02dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "prefix = \"\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"original\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"simplified\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db71fe06-a05b-4a6a-93c1-545b0c12b426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5, 2520, 4145, 766, 7, 9070, 2124, 86, 4, 1], [5, 4378, 7135, 6845, 8125, 123, 7656, 4, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[5, 2520, 4145, 766, 7, 10999, 7737, 2124, 86, 4, 1], [5, 4378, 7135, 6845, 8125, 123, 7656, 4, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afd5b4c-f5cc-4551-a414-9e61b4ca0c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f97424742748608bd685fc2daf5c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e85494a2e64d049b9cb2a09d140ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e725f18568d492a908768adb4975490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e41826-8ba7-427a-bdc4-57a761118efa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 01:46:30.169781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-24 01:46:30.169822: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-24 01:46:30.169837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-3193HLS): /proc/driver/nvidia/version does not exist\n",
      "2023-02-24 01:46:30.170650: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412c7100-7822-4624-940a-1ae7f490d0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.t5.modeling_tf_t5.TFT5ForConditionalGeneration at 0x7f58bafe77f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a6e1c49-f26f-46b6-b041-71d86874cbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 5\n",
    "\n",
    "model_name = model_name.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-xsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "599864f2-aaf5-45da-99c3-feed9c6a6e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca44f57d-b3c4-4698-a2a2-12d3483b902f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dacdaa99-cefb-4854-bd75-d11efe14fcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088aad9d-36b3-484e-907b-a17f32da77f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7543863f-e9cc-4bf7-b7c5-025c06d1e8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_predictions = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd4944c7-5fef-438e-b98a-44a5a1a16db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ! pip install transformers datasets\n",
    "# # ! pip install rouge-score nltk\n",
    "# # ! pip install huggingface_hub\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4a7b58e-d4e1-424c-bb41-36926fa7c163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e5f7859-3c76-46ab-9c81-06561eaf11a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub\n",
    "\n",
    "# !python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c7b2d9-4038-47db-a351-70d3d5526fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "12/12 [==============================] - ETA: 0s - loss: 4.1641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "2023-02-24 01:47:40.990853: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5617cdf46ea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-02-24 01:47:40.991862: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2023-02-24 01:47:41.180740: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator shared/assert_less/Assert/Assert\n",
      "2023-02-24 01:47:41.192167: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-24 01:47:41.498518: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator shared/assert_less_1/Assert/Assert\n",
      "2023-02-24 01:47:42.188238: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator shared/assert_less/Assert/Assert\n",
      "2023-02-24 01:47:55.378803: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-02-24 01:56:45.452943: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator shared/assert_less/Assert/Assert\n",
      "2023-02-24 01:56:45.645461: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator shared/assert_less_1/Assert/Assert\n",
      "2023-02-24 01:56:46.273122: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator shared/assert_less/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 659s 58s/step - loss: 4.1641 - val_loss: 4.1407 - rouge1: 0.0000e+00 - rouge2: 0.0000e+00 - rougeL: 0.0000e+00 - rougeLsum: 0.0000e+00 - gen_len: 3.8800\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 608s 55s/step - loss: 3.7339 - val_loss: 3.8323 - rouge1: 0.0000e+00 - rouge2: 0.0000e+00 - rougeL: 0.0000e+00 - rougeLsum: 0.0000e+00 - gen_len: 3.9200\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 633s 57s/step - loss: 3.5484 - val_loss: 3.7636 - rouge1: 0.0000e+00 - rouge2: 0.0000e+00 - rougeL: 0.0000e+00 - rougeLsum: 0.0000e+00 - gen_len: 4.3600\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 592s 54s/step - loss: 3.3742 - val_loss: 3.7272 - rouge1: 0.0000e+00 - rouge2: 0.0000e+00 - rougeL: 0.0000e+00 - rougeLsum: 0.0000e+00 - gen_len: 4.7200\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 614s 56s/step - loss: 3.3862 - val_loss: 3.6957 - rouge1: 0.0000e+00 - rouge2: 0.0000e+00 - rougeL: 0.0000e+00 - rougeLsum: 0.0000e+00 - gen_len: 4.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f581813c490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback, KerasMetricCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "## Well SHITS i should have read the documentation first instead of copying this shits chunk of code!\n",
    "# tensorboard_callback = TensorBoard(log_dir=\"./summarization_model_save/logs\")\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"./summarization_model_save\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     hub_model_id=push_to_hub_model_id,\n",
    "# )\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True\n",
    ")\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=validation_dataset, \n",
    "    batch_size = 16, # ??? \n",
    "    epochs=5, \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb108bef-f19c-4a36-94d3-736a65726165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T23_1000.xlsx'))\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67b5a129-2b60-4b72-9256-2c6be2e571b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['original'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42bb9cda-d3c1-48aa-959e-4855f258d6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = df[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa4cdce2-b050-42b8-ab53-b858a3638707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_texts = []\n",
    "for text in test_df['original'].tolist():\n",
    "    tokenized = tokenizer([text], return_tensors='np')\n",
    "    out = model.generate(**tokenized, max_length=128)\n",
    "    pred_texts.append(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79773595-8fa1-498e-828b-96b2fea88ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.insert(2, \"pred\", pred_texts, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "650b6fa3-97cc-4002-b5e3-4122a33a3c45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>新任の取締役として江崎優を紹介いたします。</td>\n",
       "      <td>新しい取締役として江崎優を紹介します。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>彼は一日たりとも自分のしたことを後悔せずに過ごした日はなかった。</td>\n",
       "      <td>彼はいつも自分のしたことを残念に思わないで過ごした日はなかった。</td>\n",
       "      <td>自分のしたことを後悔せずに過ごした日はなかった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>彼女は目にいっぱい浮かべて、さよならと手を振っていた。</td>\n",
       "      <td>彼女は目にいっぱいの涙ため、さようならと手を振っていた。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>金は天下の回りもの。</td>\n",
       "      <td>金は社会の回りもの。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>日が沈んでしまったからも、彼らはまだ踊りを止めなかった。</td>\n",
       "      <td>日が沈んでしまってからも、彼らはまだダンスを止めなかった。</td>\n",
       "      <td>日が沈んでも、踊りを止めなかった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>景気はいかがですか。</td>\n",
       "      <td>経済状況はどうですか。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>彼女は田んぼの方に進んだ。</td>\n",
       "      <td>彼女は植物を育てる土地の方に進んだ。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>けばけばしい奇妙な服を着ていた彼女は人込みの中でも一際目立った。</td>\n",
       "      <td>とても品の悪いおかしい服を着ていた彼女は、多くの人たちの中でも特に目についた。</td>\n",
       "      <td>人込みの中で一際目立つようにした彼女は人込みの中でも一際目立つようにした。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>彼女は高貴な生まれの女性です。</td>\n",
       "      <td>彼女はお金持ちで、品が良い家の出身です。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>キャロルは先月ボストンを訪問した。</td>\n",
       "      <td>キャロルは先月、米の有名大学が多い都市を訪れた。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>このあたりは人家がほとんどない。</td>\n",
       "      <td>このあたりは人が住む家がほとんどない。</td>\n",
       "      <td>人家がほとんどない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>最初はやさしく思われるものが、実際は難しいことがよくある。</td>\n",
       "      <td>最初は簡単に思われるものが、実際は難しいことがよくある。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>気密性でない窓ならば、水滴ができるだろう。</td>\n",
       "      <td>空気を通さない窓ならば、少しの水もでないだろう。</td>\n",
       "      <td>気密性のない窓ならば、水滴ができるだろう。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>５日間続けて雨が降った。</td>\n",
       "      <td>５日の間続けて雨が降った。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>雨天の場合は運動会を中止する。</td>\n",
       "      <td>雨の場合は運動会をやらない。</td>\n",
       "      <td>雨天の場合は運動会を中止する。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>父はふつう８時に帰宅する。</td>\n",
       "      <td>父はいつも８時に家に帰る。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>あの赤い布は「袱紗」茶道具を清めるために使う必需品なの。</td>\n",
       "      <td>あの赤い布は「袱紗」と言い、茶道具を心からきれいにするために絶対にいる品なの。</td>\n",
       "      <td>この赤い布は「紗」茶道具なの。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>すべての人がこれらの規則を守ることが必要だ。</td>\n",
       "      <td>あらゆる人がこれらの規則を守ることが必要だ。</td>\n",
       "      <td>すべての人がこれらの規則を守ることが必要だ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>彼は包みを小脇に抱えていた。</td>\n",
       "      <td>彼は袋を腕の下に抱いていた。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>彼は有り金残らず奪われた。</td>\n",
       "      <td>彼は持っていたお金を残らず取られた。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ピョンヤンとワシントンの間で何らかの妥協に至ることが不可欠だ。</td>\n",
       "      <td>北と米の間で何らかの話し合いの成果が出ることが必要だ。</td>\n",
       "      <td>ピョンヤンとワシントンの間で何らかの妥協に至る必要がある。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>廃墟の町とか、火災とか大人や子供が殺されているのとか。</td>\n",
       "      <td>建物が壊れ人が住まない町とか、火事とか大人や子供が殺されているのとか。</td>\n",
       "      <td>廃墟の町とか、廃墟の町とか。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>アレックスは何台クルマを持っているの？</td>\n",
       "      <td>アレックスは何台自動車を持っているの？</td>\n",
       "      <td>アレックスは何台クルマを持っているの?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>おじはいつも家族に波風を立てるようなことをする。</td>\n",
       "      <td>おじはいつも家族に嫌な体験をさせるようなことをする。</td>\n",
       "      <td>おじはいつも家族に波風を立てるようなことをする。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>興奮がおさまった時、話し合いが再び始まった。</td>\n",
       "      <td>感動が静かになった時、話し合いが再び始まった。</td>\n",
       "      <td>話し合いが再び始まった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>私たちは涼しい川風に吹かれた。</td>\n",
       "      <td>私たちは川で吹く涼しい風に吹かれた。</td>\n",
       "      <td>私たちは涼しい川風に吹かれた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>えへへへ、私は運痴だから、ちょっと走ると、すぐに脇腹が痛くなっちゃうの。</td>\n",
       "      <td>あっねぇ、私は運痴だから、ちょっと走ると、すぐに腹の横が痛くなっちゃうの。</td>\n",
       "      <td>私は運痴だから、ちょっと走ると、すぐに脇腹が痛くなるの。(笑) えへへへへへへへへへへへへへ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>もう一度嵐が来ていたら、私たちの村は壊滅していたでしょう。</td>\n",
       "      <td>もう１度嵐が来ていたら、私たちの村は壊れてなくなっていたでしょう。</td>\n",
       "      <td>嵐が来ていたら、私たちの村は壊滅していたでしょう。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>彼の健康は徐々に回復している。</td>\n",
       "      <td>彼の健康は少しずつ回復している。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>音楽は私達の生活を楽しくする。</td>\n",
       "      <td>音楽は私たちの生活を楽しくする。</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                original  \\\n",
       "0                  新任の取締役として江崎優を紹介いたします。   \n",
       "1       彼は一日たりとも自分のしたことを後悔せずに過ごした日はなかった。   \n",
       "2            彼女は目にいっぱい浮かべて、さよならと手を振っていた。   \n",
       "3                             金は天下の回りもの。   \n",
       "4           日が沈んでしまったからも、彼らはまだ踊りを止めなかった。   \n",
       "5                             景気はいかがですか。   \n",
       "6                          彼女は田んぼの方に進んだ。   \n",
       "7       けばけばしい奇妙な服を着ていた彼女は人込みの中でも一際目立った。   \n",
       "8                        彼女は高貴な生まれの女性です。   \n",
       "9                      キャロルは先月ボストンを訪問した。   \n",
       "10                      このあたりは人家がほとんどない。   \n",
       "11         最初はやさしく思われるものが、実際は難しいことがよくある。   \n",
       "12                 気密性でない窓ならば、水滴ができるだろう。   \n",
       "13                          ５日間続けて雨が降った。   \n",
       "14                       雨天の場合は運動会を中止する。   \n",
       "15                         父はふつう８時に帰宅する。   \n",
       "16          あの赤い布は「袱紗」茶道具を清めるために使う必需品なの。   \n",
       "17                すべての人がこれらの規則を守ることが必要だ。   \n",
       "18                        彼は包みを小脇に抱えていた。   \n",
       "19                         彼は有り金残らず奪われた。   \n",
       "20       ピョンヤンとワシントンの間で何らかの妥協に至ることが不可欠だ。   \n",
       "21           廃墟の町とか、火災とか大人や子供が殺されているのとか。   \n",
       "22                   アレックスは何台クルマを持っているの？   \n",
       "23              おじはいつも家族に波風を立てるようなことをする。   \n",
       "24                興奮がおさまった時、話し合いが再び始まった。   \n",
       "25                       私たちは涼しい川風に吹かれた。   \n",
       "26  えへへへ、私は運痴だから、ちょっと走ると、すぐに脇腹が痛くなっちゃうの。   \n",
       "27         もう一度嵐が来ていたら、私たちの村は壊滅していたでしょう。   \n",
       "28                       彼の健康は徐々に回復している。   \n",
       "29                       音楽は私達の生活を楽しくする。   \n",
       "\n",
       "                                 simplified  \\\n",
       "0                       新しい取締役として江崎優を紹介します。   \n",
       "1          彼はいつも自分のしたことを残念に思わないで過ごした日はなかった。   \n",
       "2              彼女は目にいっぱいの涙ため、さようならと手を振っていた。   \n",
       "3                                金は社会の回りもの。   \n",
       "4             日が沈んでしまってからも、彼らはまだダンスを止めなかった。   \n",
       "5                               経済状況はどうですか。   \n",
       "6                        彼女は植物を育てる土地の方に進んだ。   \n",
       "7   とても品の悪いおかしい服を着ていた彼女は、多くの人たちの中でも特に目についた。   \n",
       "8                      彼女はお金持ちで、品が良い家の出身です。   \n",
       "9                  キャロルは先月、米の有名大学が多い都市を訪れた。   \n",
       "10                      このあたりは人が住む家がほとんどない。   \n",
       "11             最初は簡単に思われるものが、実際は難しいことがよくある。   \n",
       "12                 空気を通さない窓ならば、少しの水もでないだろう。   \n",
       "13                            ５日の間続けて雨が降った。   \n",
       "14                           雨の場合は運動会をやらない。   \n",
       "15                            父はいつも８時に家に帰る。   \n",
       "16  あの赤い布は「袱紗」と言い、茶道具を心からきれいにするために絶対にいる品なの。   \n",
       "17                   あらゆる人がこれらの規則を守ることが必要だ。   \n",
       "18                           彼は袋を腕の下に抱いていた。   \n",
       "19                       彼は持っていたお金を残らず取られた。   \n",
       "20              北と米の間で何らかの話し合いの成果が出ることが必要だ。   \n",
       "21      建物が壊れ人が住まない町とか、火事とか大人や子供が殺されているのとか。   \n",
       "22                      アレックスは何台自動車を持っているの？   \n",
       "23               おじはいつも家族に嫌な体験をさせるようなことをする。   \n",
       "24                  感動が静かになった時、話し合いが再び始まった。   \n",
       "25                       私たちは川で吹く涼しい風に吹かれた。   \n",
       "26    あっねぇ、私は運痴だから、ちょっと走ると、すぐに腹の横が痛くなっちゃうの。   \n",
       "27        もう１度嵐が来ていたら、私たちの村は壊れてなくなっていたでしょう。   \n",
       "28                         彼の健康は少しずつ回復している。   \n",
       "29                         音楽は私たちの生活を楽しくする。   \n",
       "\n",
       "                                                 pred  \n",
       "0                                                      \n",
       "1                            自分のしたことを後悔せずに過ごした日はなかった。  \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                   日が沈んでも、踊りを止めなかった。  \n",
       "5                                                      \n",
       "6                                                      \n",
       "7               人込みの中で一際目立つようにした彼女は人込みの中でも一際目立つようにした。  \n",
       "8                                                      \n",
       "9                                                      \n",
       "10                                         人家がほとんどない。  \n",
       "11                                                     \n",
       "12                              気密性のない窓ならば、水滴ができるだろう。  \n",
       "13                                                     \n",
       "14                                    雨天の場合は運動会を中止する。  \n",
       "15                                                     \n",
       "16                                    この赤い布は「紗」茶道具なの。  \n",
       "17                             すべての人がこれらの規則を守ることが必要だ。  \n",
       "18                                                     \n",
       "19                                                     \n",
       "20                      ピョンヤンとワシントンの間で何らかの妥協に至る必要がある。  \n",
       "21                                     廃墟の町とか、廃墟の町とか。  \n",
       "22                                アレックスは何台クルマを持っているの?  \n",
       "23                           おじはいつも家族に波風を立てるようなことをする。  \n",
       "24                                       話し合いが再び始まった。  \n",
       "25                                    私たちは涼しい川風に吹かれた。  \n",
       "26  私は運痴だから、ちょっと走ると、すぐに脇腹が痛くなるの。(笑) えへへへへへへへへへへへへへ...  \n",
       "27                          嵐が来ていたら、私たちの村は壊滅していたでしょう。  \n",
       "28                                                     \n",
       "29                                                     "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca330b-b73b-499a-a011-15ec4fe285fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d09eb3-d635-4b9e-aa9c-1e4edc31b2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c2a87-8c7c-4fa8-8db7-aa5c9951f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ファインチューニングの実行\n",
    "!python ./examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=sonoisa/t5-base-japanese \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file=SNOW_T15_150.csv \\\n",
    "    --num_train_epochs=10 \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --save_steps=5000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --output_dir=summary_ja/ \\\n",
    "    --predict_with_generate \\\n",
    "    --use_fast_tokenizer=False \\\n",
    "    --logging_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a891767-cdd3-4123-87a2-47ccd6df21c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2343ce-06b5-410b-a769-1d09e024206a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8374d3b-1c60-45d9-9f3a-145749023d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432b8b0-9f44-4319-9a91-f61fddf70f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89a419-9598-4470-86a4-f435fda486ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5719f6-daea-4f8b-873e-a1755ea761b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea1369-7add-434f-9216-0e962df52996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d155e2-4ab5-4277-84b8-fb6aeff022a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 132180, done.\u001b[K\n",
      "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
      "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
      "remote: Total 132180 (delta 12), reused 20 (delta 4), pack-reused 132139\u001b[K\n",
      "Receiving objects: 100% (132180/132180), 126.42 MiB | 19.50 MiB/s, done.\n",
      "Resolving deltas: 100% (100030/100030), done.\n",
      "/root/code/mochiyam/simply-japanese/notebooks/transformers\n",
      "Collecting accelerate>=0.12.0\n",
      "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=1.8.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 3)) (0.1.97)\n",
      "Requirement already satisfied: protobuf in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 4)) (3.19.6)\n",
      "Requirement already satisfied: rouge-score in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: nltk in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 6)) (3.8.1)\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.4-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 8)) (1.7.1)\n",
      "Requirement already satisfied: evaluate in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from -r ./examples/pytorch/summarization/requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from accelerate>=0.12.0->-r ./examples/pytorch/summarization/requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: psutil in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from accelerate>=0.12.0->-r ./examples/pytorch/summarization/requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from accelerate>=0.12.0->-r ./examples/pytorch/summarization/requirements.txt (line 1)) (22.0)\n",
      "Requirement already satisfied: pyyaml in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from accelerate>=0.12.0->-r ./examples/pytorch/summarization/requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: pandas in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: responses<0.19 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: multiprocess in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (10.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: xxhash in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from rouge-score->-r ./examples/pytorch/summarization/requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from rouge-score->-r ./examples/pytorch/summarization/requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->-r ./examples/pytorch/summarization/requirements.txt (line 6)) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->-r ./examples/pytorch/summarization/requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: click in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from nltk->-r ./examples/pytorch/summarization/requirements.txt (line 6)) (8.1.3)\n",
      "Collecting texttable\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pyppmd<1.1.0,>=0.18.1\n",
      "  Downloading pyppmd-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.7/139.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting inflate64>=0.3.1\n",
      "  Downloading inflate64-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybcj>=0.6.0\n",
      "  Downloading pybcj-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.0/379.0 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from torch>=1.3->-r ./examples/pytorch/summarization/requirements.txt (line 8)) (4.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: filelock in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r ./examples/pytorch/summarization/requirements.txt (line 2)) (2022.7.1)\n",
      "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr, accelerate\n",
      "Successfully installed accelerate-0.16.0 brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.4 pybcj-1.0.1 pycryptodomex-3.17 pyppmd-1.0.0 pyzstd-0.15.3 texttable-1.6.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-v4zry324\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-v4zry324\n",
      "  Resolved https://github.com/huggingface/transformers to commit 04d90ac49ebd4dcff31f3bb2ccf7296ee4d724b4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (6.0)\n",
      "Requirement already satisfied: filelock in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: requests in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from transformers==4.27.0.dev0) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests->transformers==4.27.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests->transformers==4.27.0.dev0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests->transformers==4.27.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests->transformers==4.27.0.dev0) (3.4)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6624595 sha256=a498bad84bd0c3d270cc104201260c02716cc1821366e553601e8c631437364f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6fjz40n2/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.1\n",
      "    Uninstalling transformers-4.26.1:\n",
      "      Successfully uninstalled transformers-4.26.1\n",
      "Successfully installed transformers-4.27.0.dev0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: packaging in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (0.12.0)\n",
      "Requirement already satisfied: aiohttp in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "%cd transformers\n",
    "\n",
    "!pip install -r ./examples/pytorch/summarization/requirements.txt\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aff90d82-b4be-491b-8528-76c0f9f01767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
      "/tmp/ipykernel_10971/3533293254.py:10: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['text', 'summary'])\n",
    "path = \"/root/code/mochiyam/simply-japanese/notebooks\"\n",
    "path = os.path.join(path, 'SNOW_T15_150.csv')\n",
    "with open(path) as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        strs = line.split(',')\n",
    "        df = df.append({'text':strs[0] , 'summary':strs[1]}, ignore_index=True)\n",
    "\n",
    "df.to_csv('please.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a287b49-c955-4e06-a07c-92d944040795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-24 03:18:37.205585: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-24 03:18:37.800754: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 03:18:37.800856: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-24 03:18:37.800883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/root/.pyenv/versions/simply-japanese/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "02/24/2023 03:27:33 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "02/24/2023 03:27:33 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=summary_ja/runs/Feb24_03-18-39_DESKTOP-3193HLS,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=summary_ja/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['mlflow', 'tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=summary_ja/,\n",
      "save_on_each_node=False,\n",
      "save_steps=5000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "02/24/2023 03:27:34 - WARNING - datasets.builder - Using custom data configuration default-5d3216bd627c774f\n",
      "02/24/2023 03:27:34 - INFO - datasets.info - Loading Dataset Infos from /root/.pyenv/versions/simply-japanese/lib/python3.8/site-packages/datasets/packaged_modules/csv\n",
      "02/24/2023 03:27:34 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-5d3216bd627c774f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-5d3216bd627c774f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 3480.75it/s]\n",
      "02/24/2023 03:27:34 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "02/24/2023 03:27:34 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1640.32it/s]\n",
      "02/24/2023 03:27:34 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "02/24/2023 03:27:34 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 0 examples [00:00, ? examples/s]/root/.pyenv/versions/simply-japanese/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n",
      "02/24/2023 03:27:34 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-5d3216bd627c774f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 634.92it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-02-24 03:28:15,091 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-02-24 03:28:15,099 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"sonoisa/t5-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"T5Model\"\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"eos_token_ids\": [\n",
      "    1\n",
      "  ],\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 512,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:668] 2023-02-24 03:28:15,829 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-02-24 03:28:15,831 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"sonoisa/t5-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"T5Model\"\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"eos_token_ids\": [\n",
      "    1\n",
      "  ],\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 512,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-02-24 03:28:15,832 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-02-24 03:28:15,832 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-02-24 03:28:15,832 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1802] 2023-02-24 03:28:15,832 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:668] 2023-02-24 03:28:15,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-02-24 03:28:15,834 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"sonoisa/t5-base-japanese\",\n",
      "  \"architectures\": [\n",
      "    \"T5Model\"\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"eos_token_ids\": [\n",
      "    1\n",
      "  ],\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 512,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2400] 2023-02-24 03:28:15,882 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sonoisa--t5-base-japanese/snapshots/8bbcbc891dd58c0267f847d118be0d5dcfd1c78d/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:575] 2023-02-24 03:28:16,888 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"max_length\": 512,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3018] 2023-02-24 03:28:19,113 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3026] 2023-02-24 03:28:19,113 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at sonoisa/t5-base-japanese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:2676] 2023-02-24 03:28:19,837 >> Generation config file not found, using a generation config created from the model config.\n",
      "Running tokenizer on train dataset:   0%|                 | 0/1 [00:00<?, ?ba/s]02/24/2023 03:28:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-5d3216bd627c774f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4287a32aaef28f64.arrow\n",
      "Running tokenizer on train dataset: 100%|█████████| 1/1 [00:00<00:00, 23.14ba/s]\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/code/mochiyam/simply-japanese/notebooks/transformers/./examples/pytorc\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33mh/summarization/\u001b[0m\u001b[1;33mrun_summarization.py\u001b[0m:\u001b[94m741\u001b[0m in \u001b[92m<module>\u001b[0m                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m738 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m739 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m740 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m741 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m742 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/root/code/mochiyam/simply-japanese/notebooks/transformers/./examples/pytorc\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33mh/summarization/\u001b[0m\u001b[1;33mrun_summarization.py\u001b[0m:\u001b[94m568\u001b[0m in \u001b[92mmain\u001b[0m                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m training_args.do_eval:                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m566 \u001b[0m\u001b[2m│   │   \u001b[0mmax_target_length = data_args.val_max_target_length            \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mvalidation\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m raw_datasets:                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m568 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m--do_eval requires a validation dataset\u001b[0m\u001b[33m\"\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   \u001b[0meval_dataset = raw_datasets[\u001b[33m\"\u001b[0m\u001b[33mvalidation\u001b[0m\u001b[33m\"\u001b[0m]                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m570 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m data_args.max_eval_samples \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m571 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_eval_samples = \u001b[96mmin\u001b[0m(\u001b[96mlen\u001b[0m(eval_dataset), data_args.max_ev \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mValueError: \u001b[0m--do_eval requires a validation dataset\n",
      "CPU times: user 18.4 s, sys: 4.23 s, total: 22.6 s\n",
      "Wall time: 9min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ファインチューニングの実行\n",
    "!python ./examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path=sonoisa/t5-base-japanese \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --train_file=please.csv \\\n",
    "    --num_train_epochs=10 \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=4 \\\n",
    "    --save_steps=5000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --output_dir=summary_ja/ \\\n",
    "    --predict_with_generate \\\n",
    "    --use_fast_tokenizer=False \\\n",
    "    --logging_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185de0a-6864-4493-8c35-8c91aca469ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('summary_ja/')    \n",
    "tokenizer = AutoTokenizer.from_pretrained('sonoisa/t5-base-japanese') \n",
    "\n",
    "text = \"\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512,truncation=True)\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=40, min_length=10,num_beams=4, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a46f62-e93b-4b51-85b3-e806b14fb437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19379c04-cd72-4bb3-889d-c26d87fb8925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adc7c0-4af1-470d-8e62-6d3343232462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7761d2-f4bb-45f1-9649-4b35e1041eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed53172-d8dc-4657-9a51-6e2fba21a1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3075223-67b3-4b8d-8ab9-c61ee6bf8486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda4cd6-6f2e-4557-b7c1-502494061f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a07bb-0a63-4d65-83e2-dc6841ee4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pre-trained Tokenizer and T5Model both from sonoisa/t5-base-japaense\n",
    "\n",
    "model_name = \"sonoisa/t5-base-japanese\"\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)\n",
    "\n",
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_1000.xlsx'))\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"sentence\", \"#やさしい日本語\": \"label\"}, inplace=True)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.Dataset.from_dict(df)\n",
    "\n",
    "tokenized_sentence_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_sentence_data = dict(tokenized_sentence_data)\n",
    "\n",
    "tokenized_label_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_label_data = dict(tokenized_label_data)\n",
    "\n",
    "model.compile()\n",
    "\n",
    "model.fit(tokenized_sentence_data, tokenized_label_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
