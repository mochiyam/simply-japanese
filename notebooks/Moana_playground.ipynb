{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315f2705-9f10-4e2e-84a3-40179a9c0434",
   "metadata": {},
   "source": [
    "# Installations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03455e-5133-4a15-b83c-8549dfcb8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029a90-a95f-456e-a85a-d48b4e21e703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#These wheels include a copy of the MeCab library, but not a dictionary. \n",
    "#In order to use MeCab you'll need to install a dictionary. unidic-lite is a good one to start with:\n",
    "# !pip install unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61c655-fca4-4ca7-8284-aeb4a1832012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization tool\n",
    "# !pip install neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98247d2a-0bde-4094-9746-31b824c9cc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adeeaf-d7ee-4bce-b8dc-720ca4e04d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to see Japanese!\n",
    "# !pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f62505-0939-4237-ab1d-c8910f324506",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca6f550-d324-4e2c-b3f5-519c2b87dd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "import MeCab\n",
    "import neologdn\n",
    "import collections\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "#import seaborn as sns # REMINDER: make sure to remove if not using!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ab98f-2432-4694-bf22-c671dd7bc913",
   "metadata": {},
   "source": [
    "# Just having fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89616cf-a1e9-45ab-8389-9a9dd056ddca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_150.xlsx'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66411f-b4db-4785-853f-0cfc309b26d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccc51b-a9ef-4b91-b7a7-d75fc04d9436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96de15-7c9d-4f6d-ba5c-7a3c343fca53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "text = df['original'][0]\n",
    "parsed = tagger.parse(text)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e0d53-c1fd-460e-b5be-f6431ccb3f96",
   "metadata": {},
   "source": [
    "名詞 - noun\n",
    "助詞 - particle\n",
    "連体詞 - \n",
    "動詞\n",
    "補助記号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b5832-6f89-4944-a2f4-73dda1c596ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just testing stuff out\n",
    "test = MeCab.Tagger(\"-O wakati\") \n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "parsed = test.parse(text)\n",
    "print(parsed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898134a-c7ed-4bc1-accd-82952939bf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db857a77-0bef-4b8a-bcc2-ce3fb8cb4c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = MeCab.Tagger(\"r'-d /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages'\")\n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "parsed = test.parse(text)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f5a7d-39bb-4459-9d88-cd262d623da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Super dumb dumb method\n",
    "def count_all_word_frequency():\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger()\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['original']\n",
    "        node = t.parseToNode(text)\n",
    "        while node:\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "all_words = count_all_word_frequency()\n",
    "# tuples in a list\n",
    "print(all_words.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a106d0-2ddd-413c-b6eb-8c2987ed3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756c156-9908-438d-b7e6-4c25f434fde0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_word_frequency(word_freq, most_common_num):\n",
    "    freq_dist = FreqDist(word_freq)\n",
    "    freq_dist.plot(most_common_num,cumulative=False)\n",
    "#plot_word_frequency(all_words, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e25be-c729-4ba7-8c7c-9d5d416cf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super dumb dumb method\n",
    "def count_all_word_frequency():\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger()\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['original']\n",
    "        node = t.parseToNode(text)\n",
    "        while node:\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "all_words = count_all_word_frequency()\n",
    "# tuples in a list\n",
    "print(all_words.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d496b27-2583-4485-81e2-d81045a648bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = MeCab.Tagger(\"-O wakati\")\n",
    "print(text)\n",
    "text = \"あなたは何を見つめているのですか。\"\n",
    "parsed = test.parse(text)\n",
    "node = test.parseToNode(text).next\n",
    "while node.next:\n",
    "    print(node.surface, node.feature.split(',')[0])\n",
    "    node = node.next\n",
    "#node.surface.decode(\"utf-8\", \"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b65bc2-121d-4ed8-9fb5-85a9c0dc7930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#               助詞           \n",
    "#              /\n",
    "# Remove 付属語 \n",
    "#　　　　　　　 \\\n",
    "#             　 助動詞\n",
    "\n",
    "#月 が｜きれいな｜晩 でし た 。\n",
    "#付属語 : が　・　でした"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15707b5b-6e9b-4a77-b5b9-53742c43bab7",
   "metadata": {},
   "source": [
    "# With 10_000 Data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa144e1-fee7-4e81-948c-208ddf12a95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_10000.xlsx'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14a209-db7c-4e8a-9731-52b6ca1e2385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fe375-71f2-4dd6-b289-843017d49610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918856f2-9d73-4bff-9290-4f44c2cd369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counts all the independent word 自立語\n",
    "_stopwords = stopwords.words('japanese')\n",
    "\n",
    "def count_all_words(docs, col='original'):\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in docs.iterrows():\n",
    "        text = row[col]\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            part_of_speech = node.feature.split(',')[0]\n",
    "            # REPLACE_WORD_POS = (\"名詞\", \"動詞\", \"形容詞\", \"副詞\", \"未知語\") # TBD\n",
    "            # IGNORE = (\"接尾\", \"非自立\", \"代名詞\")    \n",
    "            if part_of_speech in [\"助動詞\", \"助詞\", \"補助記号\"] or node.surface in _stopwords:\n",
    "                node = node.next\n",
    "                continue\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "ind_word_freq = count_all_words(df)\n",
    "plot_word_frequency(ind_word_freq, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50c4db-7fbc-41d8-8570-61eee145858d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_2000_word_freq = ind_word_freq.most_common(2000)\n",
    "top_2000_word_freq[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe1af4-2a80-49c9-a13b-af3b1d3bb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find sentences that are exactly the same \n",
    "# 2. temp_list of tokens for sentence original and simplified\n",
    "# 3. Compare the two temp_list\n",
    "# 4. two global_lists of deleted and added(simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fc1f9-1bf6-48bb-a71c-e803a8096d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp = df.head(10)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b78a53-173a-4579-9245-07a85336c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1.  Get the corpuses that are different from original and simplified \n",
    "diff_corpus_df = df[df['original'] != df['simplified']]\n",
    "diff_corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa90c26-f8b4-4d25-8681-952e90af9d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Create a temp_list of tokens for sentence original and simplified\n",
    "original_temp_list = count_all_words(diff_corpus_df, 'original')\n",
    "simplified_temp_list = count_all_words(diff_corpus_df, 'simplified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67456dc-5cdf-400f-a019-6baa95bf35d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simplified_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622171e-99a0-45fd-8954-be66c9081ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e5072-9be0-4bd4-91d5-01b3307be918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dict(original_temp_list).items(), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0b23b-a639-4c98-b3df-d2148d8c2e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Compare the two temp_list\n",
    "\n",
    "# Collections library\n",
    "# Elements are subtracted from an iterable or from another mapping (or counter). \n",
    "# Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative.\n",
    "diff_temp_df = simplified_temp_list\n",
    "diff_temp_df.subtract(original_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be54158-304a-4ddc-ac26-249acab854f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_temp_df[diff_temp_df['count'] < 0].sort_values(by='count').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5059e4-b979-4ebf-bbbf-9fa66e970a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. two global_lists of deleted and added(simplified)\n",
    "deleted = []\n",
    "added = []\n",
    "\n",
    "diff_temp_df = pd.DataFrame(dict(diff_temp).items(), columns=['word', 'count'])\n",
    "deleted =  diff_temp_df[diff_temp_df['count'] < 0]['word'].tolist()\n",
    "added = diff_temp_df[diff_temp_df['count'] >= 0]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e38ab0-0745-4a40-94da-cc5e37065a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5dae2-6d5f-4fe4-a65d-67521ed09333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(deleted), len(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585661b7-72de-40e7-9a8c-d7eec4684138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f4a9a-2f5d-4955-9e9f-bf0c1ebad0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45642c23-3bda-4cb1-b067-472e7ce4abc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f745acf-cb82-43c6-9060-043752f5706a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827197b-1c22-4a9e-a963-602656648cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bda8ed35-eb62-47b3-a6ed-fe3837290ddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1909cc3b-3366-479f-b452-ede5bfc20c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_10000.xlsx'))\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e7e921-f425-4959-b8d7-055cdba75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LENGTH = len(df)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = DATA_LENGTH\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = DATA_LENGTH\n",
    "EMBEDDING_SIZE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0107de50-777d-4577-a3a7-12effb8905d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq : Encoder LSTM -Decoder LSTM architecture\n",
    "\n",
    "original_sentence = df['original'].to_list()\n",
    "sos_simplified_sentence = [f'<sos> {sentence}' for sentence in df['simplified'].to_list()]\n",
    "eof_simplified_sentence = df['simplified'].str.cat(['<eof>' for _ in range(DATA_LENGTH)], sep =' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bf2d1-c297-41c1-a2c9-903b9884aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip freeze | grep gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebac0f-84f7-4ccd-87f3-bcc2365d3a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048755c-2432-4782-a842-cf1f2fb8fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e1345-1743-4cfa-a757-8a1fa3ecf5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e181a05-60ce-41fa-a715-41147a9a6b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4200a01-810c-4f68-b2de-8dea95ba7be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c174087-2120-488e-92ba-1ad4a5768193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# please work\n",
    "model = word2vec.Word2Vec.load('word2vec.gensim.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58775d20-3c56-42b3-84ef-9c5302ed46b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.wv['なまえ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c966ec-e4f5-4c4e-a791-bc90c0037071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('ただいま', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902540bb-3da3-4a70-b9b2-8aa15c15028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v1 = model.wv['ただいま']\n",
    "v2 = model.wv['本日']\n",
    "res = v1 - v2\n",
    "model.wv.similarity('いま', '今')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61356774-7467-45a4-bf46-4686323d085f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082274f-4fa4-4b6b-a545-90719aa22534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list = diff_temp_df['word'].to_list()\n",
    "w2v = word2vec.Word2Vec(list, vector_size=10,\n",
    "                        window=5,\n",
    "                        min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dae01c-d78f-42ea-b68e-ace8ff709f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# w2v.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd897de-3ae6-468d-a6cd-b262af942d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b271102-2eba-48db-a353-2e0122b73a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#代名詞、名詞、動詞\n",
    "\n",
    "test = MeCab.Tagger()\n",
    "text = \"ただいま話し中です。\"\n",
    "parsed = test.parse(text)\n",
    "node = test.parseToNode(text).next\n",
    "while node.next:\n",
    "    print(node.surface, node.feature)\n",
    "    node = node.next\n",
    "#node.surface.decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6719911-26f4-4469-b1f8-93a559d06215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369a775-5d17-4789-b2fc-af9f01fa02a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb83c4-ffad-4c13-8705-ce8855704e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ac85930-6f8a-4efd-876f-e8d491b9e7fc",
   "metadata": {},
   "source": [
    "## LSTM Encoder Decoder Transformation Model... attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62550af8-f4ea-4848-acdf-ea38d9d085e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42fe2df-1e1d-41ab-92ff-8484ca234157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_sentences = df['original'].to_list()\n",
    "simplified_sentences = df['simplified'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a642347-34cd-4061-8ef9-78de284ac25a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_token \u001b[38;5;241m=\u001b[39m target_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((w, i) \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words))\n\u001b[1;32m      8\u001b[0m encoder_tokens \u001b[38;5;241m=\u001b[39m decoder_tokens \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# for Masking > 335477\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m max_encoder_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43moriginal_sentence\u001b[49m) \u001b[38;5;66;03m# > 28\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Input and output vocabulary sizes (types of words)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prepend BOS (Beginning Of Sentence) at the beginning of the sentence  \u001b[39;00m\n\u001b[1;32m     13\u001b[0m max_decoder_seq_length \u001b[38;5;241m=\u001b[39m max_encoder_seq_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# <BOS> 29\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec.gensim.model\")\n",
    "# len(words) = 335477\n",
    "words = [\"<PAD>\"] + model.wv.index_to_key\n",
    "# embedding.shape = (335477, 50)\n",
    "embedding = np.insert(model.wv.vectors, 0, 0, axis=0)\n",
    "# Dictionary of word and its index\n",
    "input_token = target_token = dict((w, i) for i, w in enumerate(words))\n",
    "encoder_tokens = decoder_tokens = embedding.shape[0] # for Masking > 335477\n",
    "max_encoder_seq_length = max(len(sentence) for sentence in original_sentence) # > 28\n",
    "\n",
    "# Input and output vocabulary sizes (types of words)\n",
    "# Prepend BOS (Beginning Of Sentence) at the beginning of the sentence  \n",
    "max_decoder_seq_length = max_encoder_seq_length + 1 # <BOS> 29\n",
    "output_dim = embedding.shape[1] # > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617ece98-1509-476a-910b-0416de628e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip list | grep tensorflow\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff420e40-1eaf-4567-9615-f851ef0b31fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 16:21:16.874562: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 16:21:17.289025: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-18 16:21:17.304671: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 16:21:17.304695: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 16:21:18.335768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 16:21:18.336078: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 16:21:18.336090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Activation, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff9f184-5dd8-44a8-8947-6228dce81d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf8e52d-9550-4fa6-8e33-cffd1295b864",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 13:41:43.205937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-18 13:41:43.206083: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-18 13:41:43.206147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-3193HLS): /proc/driver/nvidia/version does not exist\n",
      "2023-02-18 13:41:43.206922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 64) dtype=float32 (created by layer 'lstm')>,\n",
       " <KerasTensor: shape=(None, 64) dtype=float32 (created by layer 'lstm')>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dimension = 64\n",
    "\n",
    "# Embedding\n",
    "layer_emb = Embedding(input_dim=encoder_tokens,\n",
    "                      output_dim=output_dim,\n",
    "                      trainable=False,\n",
    "                      mask_zero=True)\n",
    "# Encoder\n",
    "# Input() is used to instantiate a Keras tensor\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "x = layer_emb(encoder_inputs)\n",
    "# Takes the hidden state and internal state of this Embedding layer\n",
    "# state_h : hidden state in a cell, state_c : memory cell internal state\n",
    "_, state_h, state_c = LSTM(hidden_dimension, return_sequences=True, return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc49d9d9-b963-4834-8ff0-c96a2caa65e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "x = layer_emb(decoder_inputs)\n",
    "x, _, _ = LSTM(hidden_dimension, return_sequences=True, return_state=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(decoder_tokens)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc395950-0c70-45a7-bf09-b78f1e5ece0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_masking(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(tf.gather_nd(y_true, tf.where(y_pred._keras_mask)), tf.gather_nd(y_pred, tf.where(y_pred._keras_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aea7a465-2055-41bc-8f27-31fb71fc07be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = RMSprop(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=lambda y_true, y_pred: tf.nn.softmax_cross_entropy_with_logits(tf.one_hot(tf.cast(y_true, tf.int32), num_decoder_tokens), y_pred),\n",
    "              metrics=[accuracy_masking])\n",
    "# set embedding matrix\n",
    "layer_emb.set_weights([embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "503d52cd-bf44-4730-99b3-d8705aa174b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.embedding.Embedding at 0x7fb7b4b5a970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cb87c1-064f-48a8-be02-40af8c6eac30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(original_sentences), max_encoder_seq_length))\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(original_sentences), max_decoder_seq_length))\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(original_sentences), max_decoder_seq_length))\n",
    "\n",
    "len(encoder_input_data), len(decoder_input_data), len(decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a06df847-3f38-4722-b38e-4024b4caa0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335479, 335480)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token['<BOS>'] = len(input_token)\n",
    "input_token['<EOS>'] = len(input_token) + 1\n",
    "input_token['<BOS>'], input_token['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a878a2a-7f27-4706-8928-1d38261f8827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335477, 335479)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token['<BOS>'] = len(target_token)\n",
    "target_token['<EOS>'] = len(target_token) + 1\n",
    "target_token['<BOS>'], target_token['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a76284ae-5b52-4976-9f32-e38d613481b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'疲'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (original_sentence, simplified_sentence) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(original_sentences, simplified_sentences)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(original_sentence):\n\u001b[0;32m----> 3\u001b[0m         encoder_input_data[i, t] \u001b[38;5;241m=\u001b[39m \u001b[43minput_token\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m     decoder_input_data[i, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m target_token[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<BOS>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# BOS\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(simplified_sentence):\n",
      "\u001b[0;31mKeyError\u001b[0m: '疲'"
     ]
    }
   ],
   "source": [
    "for i, (original_sentence, simplified_sentence) in enumerate(zip(original_sentences, simplified_sentences)):\n",
    "    for t, w in enumerate(original_sentence):\n",
    "        encoder_input_data[i, t] = input_token[w]\n",
    "\n",
    "    decoder_input_data[i, 0] = target_token['<BOS>'] # BOS\n",
    "    for t, w in enumerate(simplified_sentence):\n",
    "        decoder_input_data[i, t + 1] = target_token[w]\n",
    "        decoder_target_data[i, t] = target_token[w]\n",
    "    decoder_target_data[i, t + 1:] = target_token['<EOS>'] # EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd82a2-c4cd-4145-93b4-dcaa4dda0908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f3b3d-7b88-4002-8d37-313e976734b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2790892-413b-489f-8f98-534de4c8d222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bebcf-4d76-4ba7-9b47-42af9cba5713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c774b-b21d-4568-859d-ea3639a4e50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ddd8-8994-41a6-8560-1155f343a161",
   "metadata": {},
   "source": [
    "### Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d40f96-03c7-480e-ab62-68fdf9a7145a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Process the dataset\n",
    "Append SOS and EOS\n",
    "\"\"\"\n",
    "\n",
    "df[\"simplified_w_marker\"] = [f'<sos> {sentence} <eof>' for sentence in df['simplified']]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9652c259-0e6e-45f9-8601-66618355c660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab(df, col):\n",
    "    vocabulary = []\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['simplified_w_marker']\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            vocabulary.append(node.surface)\n",
    "            node = node.next\n",
    "    vocabulary = sorted(set(vocabulary)) + ['<unk>']\n",
    "    word2idx = dict((idx, vocab) for idx, vocab in enumerate(vocabulary))\n",
    "    idx2word = dict((vocab, idx) for idx, vocab in enumerate(vocabulary))\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95666cb7-18fe-4d4d-af6f-5260e6889c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40965d0a-8d36-4120-a96e-6cbb998775ab",
   "metadata": {},
   "source": [
    "### Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4016300-bec4-41bb-9042-884b537d55ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X : original sentence\n",
    "# y : simplified sentence\n",
    "X_word2idx, X_idx2word = get_vocab(df, 'original')\n",
    "y_word2idx, y_idx2word = get_vocab(df, 'simpflied_w_marker')\n",
    "\n",
    "X_train = df['original'].to_list()\n",
    "y_train = df[\"simplified_w_marker\"].to_list()\n",
    "\n",
    "X_vocab_size = len(X_word2idx)\n",
    "y_vocab_size = len(y_word2idx)\n",
    "\n",
    "hidden_dimension = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa90c8b4-f0d1-4526-95b6-3eb467093ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Embedding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layer_emb \u001b[38;5;241m=\u001b[39m Embedding(input_dim\u001b[38;5;241m=\u001b[39mencoder_tokens,\n\u001b[0;32m----> 3\u001b[0m                       output_dim\u001b[38;5;241m=\u001b[39m\u001b[43moutput_dim\u001b[49m,\n\u001b[1;32m      4\u001b[0m                       trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                       mask_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Input() is used to instantiate a Keras tensor\u001b[39;00m\n\u001b[1;32m      8\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "layer_emb = Embedding(input_dim=encoder_tokens,\n",
    "                      output_dim=output_dim,\n",
    "                      trainable=False,\n",
    "                      mask_zero=True)\n",
    "# Encoder\n",
    "# Input() is used to instantiate a Keras tensor\n",
    "encoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "# x = layer_emb(encoder_inputs)\n",
    "\n",
    "# Takes the hidden state and internal state of this Embedding layer\n",
    "# state_h : hidden state in a cell, state_c : memory cell internal state\n",
    "encoder_lstm = LSTM(hidden_dimension, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None,), dtype=tf.int32)\n",
    "# x = layer_emb(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_dimension, return_sequences=True, return_state=True)\n",
    "decoder_dense_layer = Dense(y_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1bb91a3-51e3-48fd-b143-a89287abc084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array(original_sentences)\n",
    "y = np.array(simplified_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3377b1b5-f9d7-4660-948d-6f18107b8ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(df)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "is_train = np.random.uniform(size=(len(df),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X[is_train], y[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X[~is_train], y[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b0cfc39c-2141-4b28-bc0f-229f344a3a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.strings.unicode_decode(X, 'UTF-8').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e05e6d8-d896-4885-a7d4-ac4f8313d52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\xe5\\xbd\\xbc\\xe3\\x81\\x8c\\xe9\\x83\\xa8\\xe5\\xb1\\x8b\\xe3\\x81\\xab\\xe3\\x81\\xaf\\xe3\\x81\\x84\\xe3\\x82\\x8b\\xe3\\x81\\xae\\xe3\\x82\\x92\\xe8\\xa6\\x8b\\xe3\\x81\\x9f\\xe3\\x80\\x82'\n",
      " b'\\xe7\\xa7\\x81\\xe9\\x81\\x94\\xe3\\x81\\xaf\\xe3\\x81\\x9d\\xe3\\x81\\xae\\xe3\\x83\\x93\\xe3\\x83\\xab\\xe5\\x85\\xa8\\xe9\\x83\\xa8\\xe3\\x82\\x92\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe3\\x81\\xab\\xe4\\xbd\\xbf\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\x8d\\xe3\\x81\\x9f\\xe3\\x80\\x82'\n",
      " b'\\xe3\\x81\\x9d\\xe3\\x82\\x8c\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe6\\xad\\xbb\\xe8\\xaa\\x9e\\xe3\\x81\\x98\\xe3\\x82\\x83\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x81\\xae\\xe3\\x80\\x82'\n",
      " b'\\xe3\\x81\\xa7\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe3\\x81\\x93\\xe3\\x81\\x93\\xe3\\x81\\xa7\\xe3\\x81\\x94\\xe8\\xaa\\xac\\xe6\\x98\\x8e\\xe3\\x81\\x97\\xe3\\x81\\xbe\\xe3\\x81\\x97\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x80\\x82'\n",
      " b'\\xe4\\xbb\\x8a\\xe6\\x99\\xa9\\xe3\\x83\\x91\\xe3\\x83\\xbc\\xe3\\x83\\x86\\xe3\\x82\\xa3\\xe3\\x83\\xbc\\xe3\\x82\\x92\\xe3\\x81\\x97\\xe3\\x81\\xbe\\xe3\\x81\\x99\\xe3\\x80\\x82'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'\\xe5\\xbd\\xbc\\xe3\\x81\\x8c\\xe9\\x83\\xa8\\xe5\\xb1\\x8b\\xe3\\x81\\xab\\xe3\\x81\\xaf\\xe3\\x81\\x84\\xe3\\x82\\x8b\\xe3\\x81\\xae\\xe3\\x82\\x92\\xe8\\xa6\\x8b\\xe3\\x81\\x9f\\xe3\\x80\\x82'\n",
      " b'\\xe7\\xa7\\x81\\xe3\\x81\\x9f\\xe3\\x81\\xa1\\xe3\\x81\\xaf\\xe3\\x81\\x9d\\xe3\\x81\\xae\\xe3\\x83\\x93\\xe3\\x83\\xab\\xe5\\x85\\xa8\\xe3\\x81\\xa6\\xe3\\x82\\x92\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe3\\x81\\xab\\xe4\\xbd\\xbf\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\x8d\\xe3\\x81\\x9f\\xe3\\x80\\x82'\n",
      " b'\\xe3\\x81\\x9d\\xe3\\x82\\x8c\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe6\\xad\\xbb\\xe3\\x82\\x93\\xe3\\x81\\xa0\\xe8\\xa8\\x80\\xe8\\x91\\x89\\xe3\\x81\\x98\\xe3\\x82\\x83\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x81\\xae\\xe3\\x80\\x82'\n",
      " b'\\xe3\\x81\\xa7\\xe3\\x81\\xaf\\xe3\\x80\\x81\\xe3\\x81\\x93\\xe3\\x81\\x93\\xe3\\x81\\xa7\\xe3\\x81\\x94\\xe8\\xaa\\xac\\xe6\\x98\\x8e\\xe3\\x81\\x97\\xe3\\x81\\xbe\\xe3\\x81\\x97\\xe3\\x82\\x87\\xe3\\x81\\x86\\xe3\\x80\\x82'\n",
      " b'\\xe4\\xbb\\x8a\\xe6\\x97\\xa5\\xe3\\x81\\xae\\xe5\\xa4\\x9c\\xe3\\x83\\x91\\xe3\\x83\\xbc\\xe3\\x83\\x86\\xe3\\x82\\xa3\\xe3\\x83\\xbc\\xe3\\x82\\x92\\xe3\\x81\\x97\\xe3\\x81\\xbe\\xe3\\x81\\x99\\xe3\\x80\\x82'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5206b388-5d8b-43bc-aaf3-b79e6ea679ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "      <th>simplified_w_marker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>父は私が外国へ行くことを承知した。</td>\n",
       "      <td>父は私が外国へ行くことを許した。</td>\n",
       "      <td>&lt;sos&gt; 父は私が外国へ行くことを許した。 &lt;eof&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>卑屈な奴。</td>\n",
       "      <td>自分のことをダメだと考える人。</td>\n",
       "      <td>&lt;sos&gt; 自分のことをダメだと考える人。 &lt;eof&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>それは本当のはずはない。</td>\n",
       "      <td>それは本当のはずはない。</td>\n",
       "      <td>&lt;sos&gt; それは本当のはずはない。 &lt;eof&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>車がそんなに混んでなければ問題ないでしょう。</td>\n",
       "      <td>車がそんなに混んでなければ問題ないでしょう。</td>\n",
       "      <td>&lt;sos&gt; 車がそんなに混んでなければ問題ないでしょう。 &lt;eof&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>２時間も待たされた。</td>\n",
       "      <td>２時間も待った。</td>\n",
       "      <td>&lt;sos&gt; ２時間も待った。 &lt;eof&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 original              simplified  \\\n",
       "0       父は私が外国へ行くことを承知した。        父は私が外国へ行くことを許した。   \n",
       "1                   卑屈な奴。         自分のことをダメだと考える人。   \n",
       "2            それは本当のはずはない。            それは本当のはずはない。   \n",
       "3  車がそんなに混んでなければ問題ないでしょう。  車がそんなに混んでなければ問題ないでしょう。   \n",
       "4              ２時間も待たされた。                ２時間も待った。   \n",
       "\n",
       "                  simplified_w_marker  \n",
       "0        <sos> 父は私が外国へ行くことを許した。 <eof>  \n",
       "1         <sos> 自分のことをダメだと考える人。 <eof>  \n",
       "2            <sos> それは本当のはずはない。 <eof>  \n",
       "3  <sos> 車がそんなに混んでなければ問題ないでしょう。 <eof>  \n",
       "4                <sos> ２時間も待った。 <eof>  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48c9ad37-0b3b-44d9-86ec-1d990cbd4de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe7\\x88\\xb6\\xe3\\x81\\xaf\\xe7\\xa7\\x81\\xe3\\x81\\x8c\\xe5\\xa4\\x96\\xe5\\x9b\\xbd\\xe3\\x81\\xb8\\xe8\\xa1\\x8c\\xe3\\x81\\x8f\\xe3\\x81\\x93\\xe3\\x81\\xa8\\xe3\\x82\\x92\\xe6\\x89\\xbf\\xe7\\x9f\\xa5\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe3\\x80\\x82'\n",
      "b'\\xe7\\x88\\xb6\\xe3\\x81\\xaf\\xe7\\xa7\\x81\\xe3\\x81\\x8b\\xe3\\x82\\x99\\xe5\\xa4\\x96\\xe5\\x9b\\xbd\\xe3\\x81\\xb8\\xe8\\xa1\\x8c\\xe3\\x81\\x8f\\xe3\\x81\\x93\\xe3\\x81\\xa8\\xe3\\x82\\x92\\xe6\\x89\\xbf\\xe7\\x9f\\xa5\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe3\\x80\\x82'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow_text as tf_text\n",
    "example_text = tf.constant('父は私が外国へ行くことを承知した。')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7a7970f-8d29-40ce-befb-1c61f1a092a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5e81f330-b93a-4ce4-a37b-e035b06ef8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "父は私が外国へ行くことを承知した。\n",
      "[START]  [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46d0ed-fdab-4acb-8da8-0a8bd5e7a211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08c747-4018-450c-94dc-4b327cd19eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b79cd-9366-48ab-9fce-7bc7052b9a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c65a0-39e8-48e2-a096-d694c364a75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb9820-428f-44b7-8d98-e38b304a7530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06f819-8ccf-4d46-906c-5063104e346e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bea09f-1055-45a8-8473-b515740ac9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-18 17:59:07.451886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 17:59:07.806393: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-18 17:59:07.827160: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 17:59:07.827200: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 17:59:08.821868: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 17:59:08.822026: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 17:59:08.822033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████████████████████████████████████| 1.96k/1.96k [00:00<00:00, 343kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████| 710/710 [00:00<00:00, 158kB/s]\n",
      "Downloading (…)\"spiece.model\";: 100%|████████████████████████████████████████████████| 804k/804k [00:00<00:00, 12.8MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████| 1.79k/1.79k [00:00<00:00, 1.45MB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███████████████████████████████████████████| 892M/892M [00:13<00:00, 65.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sonoisa/t5-base-japanese\")\n",
    "model = AutoModel.from_pretrained(\"sonoisa/t5-base-japanese\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
