{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315f2705-9f10-4e2e-84a3-40179a9c0434",
   "metadata": {},
   "source": [
    "# Installations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03455e-5133-4a15-b83c-8549dfcb8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029a90-a95f-456e-a85a-d48b4e21e703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#These wheels include a copy of the MeCab library, but not a dictionary. \n",
    "#In order to use MeCab you'll need to install a dictionary. unidic-lite is a good one to start with:\n",
    "!pip install unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61c655-fca4-4ca7-8284-aeb4a1832012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization tool\n",
    "!pip install neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98247d2a-0bde-4094-9746-31b824c9cc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adeeaf-d7ee-4bce-b8dc-720ca4e04d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to see Japanese!\n",
    "!pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f62505-0939-4237-ab1d-c8910f324506",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ca6f550-d324-4e2c-b3f5-519c2b87dd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "import MeCab\n",
    "import neologdn\n",
    "import collections\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "#import seaborn as sns # REMINDER: make sure to remove if not using!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ab98f-2432-4694-bf22-c671dd7bc913",
   "metadata": {},
   "source": [
    "# Just having fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89616cf-a1e9-45ab-8389-9a9dd056ddca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_150.xlsx'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66411f-b4db-4785-853f-0cfc309b26d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccc51b-a9ef-4b91-b7a7-d75fc04d9436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96de15-7c9d-4f6d-ba5c-7a3c343fca53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "text = df['original'][0]\n",
    "parsed = tagger.parse(text)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e0d53-c1fd-460e-b5be-f6431ccb3f96",
   "metadata": {},
   "source": [
    "名詞 - noun\n",
    "助詞 - particle\n",
    "連体詞 - \n",
    "動詞\n",
    "補助記号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b5832-6f89-4944-a2f4-73dda1c596ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just testing stuff out\n",
    "test = MeCab.Tagger(\"-O wakati\")\n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "parsed = test.parse(text)\n",
    "print(parsed.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898134a-c7ed-4bc1-accd-82952939bf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db857a77-0bef-4b8a-bcc2-ce3fb8cb4c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = MeCab.Tagger(\"r'-d /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages'\")\n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "parsed = test.parse(text)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f5a7d-39bb-4459-9d88-cd262d623da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Super dumb dumb method\n",
    "def count_all_word_frequency():\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger()\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['original']\n",
    "        node = t.parseToNode(text)\n",
    "        while node:\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "all_words = count_all_word_frequency()\n",
    "# tuples in a list\n",
    "print(all_words.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a106d0-2ddd-413c-b6eb-8c2987ed3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756c156-9908-438d-b7e6-4c25f434fde0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_word_frequency(word_freq, most_common_num):\n",
    "    freq_dist = FreqDist(word_freq)\n",
    "    freq_dist.plot(most_common_num,cumulative=False)\n",
    "plot_word_frequency(all_words, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e25be-c729-4ba7-8c7c-9d5d416cf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super dumb dumb method\n",
    "def count_all_word_frequency():\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger()\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['original']\n",
    "        node = t.parseToNode(text)\n",
    "        while node:\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "all_words = count_all_word_frequency()\n",
    "# tuples in a list\n",
    "print(all_words.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d496b27-2583-4485-81e2-d81045a648bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = MeCab.Tagger(\"-O wakati\")\n",
    "print(text)\n",
    "text = \"あなたは何を見つめているのですか。\"\n",
    "parsed = test.parse(text)\n",
    "node = test.parseToNode(text).next\n",
    "while node.next:\n",
    "    print(node.surface, node.feature.split(',')[0])\n",
    "    node = node.next\n",
    "#node.surface.decode(\"utf-8\", \"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b65bc2-121d-4ed8-9fb5-85a9c0dc7930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#               助詞           \n",
    "#              /\n",
    "# Remove 付属語 \n",
    "#　　　　　　　 \\\n",
    "#             　 助動詞\n",
    "\n",
    "#月 が｜きれいな｜晩 でし た 。\n",
    "#付属語 : が　・　でした"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15707b5b-6e9b-4a77-b5b9-53742c43bab7",
   "metadata": {},
   "source": [
    "# With 10_000 Data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa144e1-fee7-4e81-948c-208ddf12a95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_10000.xlsx'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14a209-db7c-4e8a-9731-52b6ca1e2385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fe375-71f2-4dd6-b289-843017d49610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918856f2-9d73-4bff-9290-4f44c2cd369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counts all the independent word 自立語\n",
    "_stopwords = stopwords.words('japanese')\n",
    "\n",
    "def count_all_words(docs, col='original'):\n",
    "    all_words = collections.Counter()\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in docs.iterrows():\n",
    "        text = row[col]\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            part_of_speech = node.feature.split(',')[0]\n",
    "            if part_of_speech in [\"助動詞\", \"助詞\", \"補助記号\"] or node.surface in _stopwords:\n",
    "                node = node.next\n",
    "                continue\n",
    "            all_words[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_words\n",
    "ind_word_freq = count_all_words(df)\n",
    "plot_word_frequency(ind_word_freq, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50c4db-7fbc-41d8-8570-61eee145858d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_2000_word_freq = ind_word_freq.most_common(2000)\n",
    "top_2000_word_freq[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe1af4-2a80-49c9-a13b-af3b1d3bb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find sentences that are exactly the same \n",
    "# 2. temp_list of tokens for sentence original and simplified\n",
    "# 3. Compare the two temp_list\n",
    "# 4. two global_lists of deleted and added(simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fc1f9-1bf6-48bb-a71c-e803a8096d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp = df.head(10)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b78a53-173a-4579-9245-07a85336c2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1.  Get the corpuses that are different from original and simplified \n",
    "diff_corpus_df = df[df['original'] != df['simplified']]\n",
    "diff_corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa90c26-f8b4-4d25-8681-952e90af9d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Create a temp_list of tokens for sentence original and simplified\n",
    "original_temp_list = count_all_words(diff_corpus_df, 'original')\n",
    "simplified_temp_list = count_all_words(diff_corpus_df, 'simplified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67456dc-5cdf-400f-a019-6baa95bf35d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simplified_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622171e-99a0-45fd-8954-be66c9081ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e5072-9be0-4bd4-91d5-01b3307be918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(dict(original_temp_list).items(), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0b23b-a639-4c98-b3df-d2148d8c2e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Compare the two temp_list\n",
    "\n",
    "# Collections library\n",
    "# Elements are subtracted from an iterable or from another mapping (or counter). \n",
    "# Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative.\n",
    "diff_temp = simplified_temp_list\n",
    "diff_temp.subtract(original_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be54158-304a-4ddc-ac26-249acab854f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_temp_df[diff_temp_df['count'] < 0].sort_values(by='count').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5059e4-b979-4ebf-bbbf-9fa66e970a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. two global_lists of deleted and added(simplified)\n",
    "deleted = []\n",
    "added = []\n",
    "\n",
    "diff_temp_df = pd.DataFrame(dict(diff_temp).items(), columns=['word', 'count'])\n",
    "deleted =  diff_temp_df[diff_temp_df['count'] < 0]['word'].tolist()\n",
    "added = diff_temp_df[diff_temp_df['count'] >= 0]['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e38ab0-0745-4a40-94da-cc5e37065a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5dae2-6d5f-4fe4-a65d-67521ed09333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(deleted), len(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585661b7-72de-40e7-9a8c-d7eec4684138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f4a9a-2f5d-4955-9e9f-bf0c1ebad0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "517e73a7-50a3-487a-839f-2572734eb7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>父は私が外国へ行くことを承知した。</td>\n",
       "      <td>父は私が外国へ行くことを許した。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>卑屈な奴。</td>\n",
       "      <td>自分のことをダメだと考える人。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>それは本当のはずはない。</td>\n",
       "      <td>それは本当のはずはない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>車がそんなに混んでなければ問題ないでしょう。</td>\n",
       "      <td>車がそんなに混んでなければ問題ないでしょう。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>２時間も待たされた。</td>\n",
       "      <td>２時間も待った。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>彼女は服にたくさん金を使う。</td>\n",
       "      <td>彼女は服に多くの金を使う。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>彼は長年、腰痛で困っている。</td>\n",
       "      <td>彼は長い間、腰が痛くて困っている。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>彼は公園のどこかにいる。</td>\n",
       "      <td>彼は公園のどこかにいる。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>子供は大人の父なり。</td>\n",
       "      <td>子供は大人の父なり。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>私は時間を持て余している。</td>\n",
       "      <td>私は時間がたくさんある。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    original              simplified\n",
       "0          父は私が外国へ行くことを承知した。        父は私が外国へ行くことを許した。\n",
       "1                      卑屈な奴。         自分のことをダメだと考える人。\n",
       "2               それは本当のはずはない。            それは本当のはずはない。\n",
       "3     車がそんなに混んでなければ問題ないでしょう。  車がそんなに混んでなければ問題ないでしょう。\n",
       "4                 ２時間も待たされた。                ２時間も待った。\n",
       "...                      ...                     ...\n",
       "9995          彼女は服にたくさん金を使う。           彼女は服に多くの金を使う。\n",
       "9996          彼は長年、腰痛で困っている。       彼は長い間、腰が痛くて困っている。\n",
       "9997            彼は公園のどこかにいる。            彼は公園のどこかにいる。\n",
       "9998              子供は大人の父なり。              子供は大人の父なり。\n",
       "9999           私は時間を持て余している。            私は時間がたくさんある。\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/root/code/mochiyam/simply-japanese/data/2_RawData\"\n",
    "df = pd.read_excel(os.path.join(path, 'SNOW_T15_10000.xlsx'))\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8ed35-eb62-47b3-a6ed-fe3837290ddb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909cc3b-3366-479f-b452-ede5bfc20c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107de50-777d-4577-a3a7-12effb8905d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq : Encoder LSTM -Decoder LSTM architecture\n",
    "\n",
    "original_sentence = df['original'].to_list()\n",
    "sos_simplified_sentence = [f'<sos> {sentence}' for sentence in df['simplified'].to_list()]\n",
    "eof_simplified_sentence = df['simplified'].str.cat(['<eof>' for _ in range(DATA_LENGTH)], sep =' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7e921-f425-4959-b8d7-055cdba75e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LENGTH = len(df)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = DATA_LENGTH\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = DATA_LENGTH\n",
    "EMBEDDING_SIZE = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
