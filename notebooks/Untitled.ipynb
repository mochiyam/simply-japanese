{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04eecb0-8de6-4d5c-9e0a-de059928df2e",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327925a-fb15-457b-91b7-b9d283bcce2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Information on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bf8aa-48f5-4c32-ab86-7cacc1cea3a9",
   "metadata": {},
   "source": [
    "Data Fields for SNOW T15 and SNOW T23 ⛄<br>\n",
    "Resource: https://huggingface.co/datasets/snow_simplified_japanese_corpus <br>\n",
    "Paper: https://aclanthology.org/L18-1072.pdf\n",
    "\n",
    "- <strong>ID</strong>: sentence ID.\n",
    "- <strong>original_ja</strong>: original Japanese sentebolnce.\n",
    "- <strong>simplified_ja</strong>: simplified Japanese sentence.\n",
    "- <strong>original_en</strong>: original English sentence.\n",
    "- <strong>proper_noun</strong>: (included ONLY in SNOW T23) Proper nowus that the workers has extracted as proper nouns. The authors instructed workers not to rewrite proper nouns, leaving the determination of proper nouns to the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65e0c0-f8b3-4be5-8012-11c91ba07221",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef06295-331e-4e2e-84ee-538e9aead275",
   "metadata": {},
   "source": [
    "In the SNOW T15 dataset it states: <br>\n",
    "<i>Core vocabulary is restricted to 2,000 words where it is selected by accounting for several factors such as meaning preservation, variation, simplicity and the UniDic word segmentation criterion/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae1fe8-1d47-48ab-ac7f-70e4c363431e",
   "metadata": {},
   "source": [
    "#### Step 1: Take a sample size from the SNOW T15 dataset and extracted 2,000 simplified terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edd34b4-c125-445c-ace0-a1b1c130f513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (1.0.6)\n",
      "Requirement already satisfied: unidic-lite in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (1.0.8)\n",
      "Requirement already satisfied: neologdn in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: openpyxl in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: et-xmlfile in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: japanize_matplotlib in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: matplotlib in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from japanize_matplotlib) (3.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->japanize_matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Required installations \"\"\"\n",
    "\n",
    "!pip install mecab-python3\n",
    "#These wheels include a copy of the MeCab library, but not a dictionary. \n",
    "#In order to use MeCab you'll need to install a dictionary. unidic-lite is a good one to start with:\n",
    "!pip install unidic-lite\n",
    "\n",
    "# normalization tool\n",
    "!pip install neologdn\n",
    "\n",
    "!pip install openpyxl\n",
    "\n",
    "# To be able to see in Japanese!\n",
    "!pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f78cb4-2298-4c4b-9bcb-cd7ea9ac4218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "import MeCab\n",
    "import neologdn\n",
    "import collections\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a2576f-7fac-48bc-94cd-c8edfbb1a1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    \"\"\"\n",
    "    Gets csv data under 'simply-japanese/data/'\n",
    "    Returns as Dataframe where columns=['original','simplified']\n",
    "    \"\"\"\n",
    "\n",
    "    # FIXME:  Make sure to\n",
    "    # 1. Change these when you transfer to .py file\n",
    "    # 2. Put these global variables somewhere else\n",
    "    \n",
    "    CURRENT_PATH = 'notebooks/Untitled.ipynb'\n",
    "    DATA_PATH = 'data/2_RawData'\n",
    "    csv_path = os.path.abspath(__file__)[:-len(CURRENT_PATH)]  + DATA_PATH\n",
    "    df = pd.read_excel(os.path.join(csv_path, file))\n",
    "    \n",
    "    df.drop(columns=['#英語(原文)','#固有名詞'], inplace=True, errors='ignore')\n",
    "    df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e798266-15fb-4aaa-a9fd-e348d22802d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FIXME: Set df in __init__ \n",
    "def term_frequency(df, col='original'):\n",
    "    \"\"\"\n",
    "    Count number of terms in a corpus\n",
    "    Ignore independent words  [\"助動詞\", \"助詞\", \"補助記号\"] and words in japanese stopwords\n",
    "    Returns collection of term and its frequency\n",
    "    \"\"\"\n",
    "    # FIXME : Need to find a way to implement japanese_stopword.txt when this file is used externally\n",
    "    jp_stopwords = stopwords.words('japanese')\n",
    "    all_terms = collections.Counter()\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row[col]\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            part_of_speech = node.feature.split(',')[0]\n",
    "            if part_of_speech in [\"助動詞\", \"助詞\", \"補助記号\"] or node.surface in jp_stopwords:\n",
    "                node = node.next\n",
    "                continue\n",
    "            all_terms[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f1317e0-a444-48ee-96a5-c6c5394cb5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_simplified_terms(df, idx):\n",
    "    \"\"\"\n",
    "    Only returns simplified terms that exists in the simplified column\n",
    "    Return list until the top 'n' elements from most common\n",
    "    \"\"\"\n",
    "    # Filter out corpuses if original and simplified are exactly the same\n",
    "    diff_corpus_df = df[df['original'] != df['simplified']]\n",
    "    \n",
    "    # Create collections of original and simplified terms\n",
    "    original_terms = term_frequency(diff_corpus_df, 'original')\n",
    "    simplified_terms = term_frequency(diff_corpus_df, 'simplified')\n",
    "    \n",
    "    # Compare two collections using subtract\n",
    "    diff_terms = simplified_terms\n",
    "    diff_terms.subtract(original_terms)\n",
    "    \n",
    "    diff_terms_df = pd.DataFrame(dict(diff_terms).items(), columns=['word', 'count'])\n",
    "    return diff_terms_df[diff_terms_df['count'] >= 0].sort_values(by='count', ascending=False)['word'].tolist()[:idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f57e2f-4d1f-4046-b262-2070dcfff634",
   "metadata": {},
   "source": [
    "#### Step 2: Using the 2000 list of simplified terms from Step 1, find the nearest term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1de6d-b5d1-4c51-ab1d-c8523a09271c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1480f8-65dd-4ae6-ad4e-709b0f74a9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee45dff-f5a8-4e6e-862c-a285e6b17324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4718bb-f2c9-4c7e-8c20-3a96f7335553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b31f291a-69ca-4a79-99b2-03e18f7ded63",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6fd1b-fb33-408a-969b-ec5fff3d87c5",
   "metadata": {},
   "source": [
    "# 3.1) Data Organization and Clean Up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8eacf-b13a-4b4f-8a89-ac83ce9db324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imported libraries go here for Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15aed2-a563-4d94-89b3-97a81772597c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb2145ed41b1af7fe232120fa962063b18150619d590444428e8892237256527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
