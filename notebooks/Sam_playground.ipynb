{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04d0d4a-eb59-412e-badf-dff1a1b74c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install mecab-python3\n",
    "#These wheels include a copy of the MeCab library, but not a dictionary. \n",
    "#In order to use MeCab you'll need to install a dictionary. unidic-lite is a good one to start with:\n",
    "# !pip install unidic-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0ba96a-a4ac-4a45-9a0f-48de5898e38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization tool\n",
    "# !pip install neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4abbff6-adb4-474e-b92a-6d59c744aa8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757ad56a-eede-439b-9581-0b940dff1ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # To be able to see Japanese!\n",
    "# !pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be0b4b-4e1e-42db-935b-100e95db2407",
   "metadata": {
    "tags": []
   },
   "source": [
    "everything was deleted what a christmas miracle....\n",
    "lets start again ...\n",
    "\n",
    "\n",
    "## Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba8a53d-8b81-449d-83da-d9d94f726dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "import MeCab\n",
    "import neologdn\n",
    "import collections\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "#import seaborn as sns # REMINDER: make sure to remove if not using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83dc8467-21cf-4d4b-abf7-07fb4be1df4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>simplified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>彼女は通りを横切った。</td>\n",
       "      <td>彼女は通りを横に通っていった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>私が知るかぎり彼女は大変よい人だ。</td>\n",
       "      <td>私が知る限り彼女は大変よい人だ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>私のクラスの少女たちはみんな親切だ。</td>\n",
       "      <td>私のクラスの少女たちはみんな親切だ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>彼は試験に合格できなかった。</td>\n",
       "      <td>彼は試験に合格できなかった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>彼女はあなたにあえて喜ぶでしょうね。</td>\n",
       "      <td>彼女はあなたに会うことができて喜ぶでしょうね。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             original               simplified\n",
       "0         彼女は通りを横切った。          彼女は通りを横に通っていった。\n",
       "1   私が知るかぎり彼女は大変よい人だ。         私が知る限り彼女は大変よい人だ。\n",
       "2  私のクラスの少女たちはみんな親切だ。       私のクラスの少女たちはみんな親切だ。\n",
       "3      彼は試験に合格できなかった。           彼は試験に合格できなかった。\n",
       "4  彼女はあなたにあえて喜ぶでしょうね。  彼女はあなたに会うことができて喜ぶでしょうね。"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'Combined_85K_150.xlsx'\n",
    "df = pd.read_excel(r'/home/samuelhenderson/code/simply-japanese/data/2_RawData/' + file_name)\n",
    "df.drop(columns=['#英語(原文)'], inplace=True)\n",
    "df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54cfef5-d120-4fca-81fd-49486d035817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cosine similarity \n",
    "# X= df['original']\n",
    "# y= df['simplified']\n",
    "\n",
    "# cosine_similarity(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b99331bc-d05b-4cce-ae4a-450771e18aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing out stuff from github repository : \n",
    "# https://github.com/ZHAOTING/Japanese-sentence-similarity-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b1c206c-99d3-4d6e-aba1-c4a4b9cdcc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m unidic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1dd08b-ea4f-43b8-9974-4811e95b3061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #fast text\n",
    "# import fasttext\n",
    "# import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f253da11-b0fa-4cd5-8606-8bd17d9ca319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext.util.download_model('jp', if_exists='ignore')  # English\n",
    "# ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3640525-e8e0-48d7-a40c-f26d2fb1b221",
   "metadata": {},
   "source": [
    "## SumEval & Bleu caluculator (Sacrebleu)\n",
    "-------------------------- can come back to this but WER sounds more promising... --------------------------\n",
    "\n",
    "https://github.com/chakki-works/sumeval#welcome-contribution-tada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0a8ac1-6eb7-4e1c-9af5-26d103f3bd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e031400-476d-487e-8767-8c96d8232cc5",
   "metadata": {},
   "source": [
    "Trying to get the right dependency for the sumeval & Bleu caluculator (Sacrebleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d392c72d-0fce-4c7a-8045-9d40d1a6f2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # pip install sumeval==0.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45fcfda2-645b-43d6-8f58-1535ed0ae7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip uninstall sumeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2de4c4b-4954-4bb9-abdb-c2fef23976b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8190ab8c-a2c5-4e05-8658-2e029aaca2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eea230c-87f0-4d3e-85cb-b33acd30775a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu, DEFAULT_TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08bc3c6b-c7b4-4c6a-a566-aefe500905aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.iloc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac8b694d-4296-4e21-8127-bb10b9e767e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sumeval.metrics.bleu import BLEUCalculator\n",
    "# # https://github.com/chakki-works/sumeval\n",
    "# from janome.tokenizer import Tokenizer\n",
    "\n",
    "# # bleu calculator for english\n",
    "# # bleu = BLEUCalculator()\n",
    "# # score = bleu.bleu(\"I am waiting on the beach\",\n",
    "# #                   \"He is walking on the beach\")\n",
    "\n",
    "# # bleu calculator for Japanese\n",
    "# bleu_ja = BLEUCalculator(lang=\"ja\")\n",
    "# score_ja = bleu_ja.bleu(df.iloc[0][0], df.iloc[0][1])\n",
    "# tokenizer_ja()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6329473b-35b7-403e-b8cb-4d493969fa2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sumeval.metrics.bleu import BLEUCalculator\n",
    "\n",
    "# bleu = BLEUCalculator()\n",
    "# score = bleu.bleu(\"I am waiting on the beach\",\n",
    "#                   \"He is walking on the beach\")\n",
    "\n",
    "# # bleu_ja = BLEUCalculator(lang=\"ja\")\n",
    "# # score_ja = bleu_ja.bleu(\"私はビーチで待ってる\", \"彼がベンチで待ってる\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6095541-ba51-4936-82b0-e405b62a9f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sacrebleu import ja_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "729e3ca2-8166-4352-9708-2a654c3eab47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bleu = BLEUCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25114726-fd99-4add-ba04-1bc24041cab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb3aec6c-1e5c-496e-b889-7540da0e0075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "# bleu_ja = BLEUCalculator(lang=\"ja\")\n",
    "# score_ja = bleu_ja.bleu(df.iloc[0][0], df.iloc[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae464aa-626e-47fb-bfef-9d961e674c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b110f30e-94d8-4007-b6fc-011e8066d670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sacrebleu' has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msacrebleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://github.com/chakki-works/sumeval#welcome-contribution-tada\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sacrebleu' has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "sacrebleu.info()\n",
    "# https://github.com/chakki-works/sumeval#welcome-contribution-tada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24b895-bcd9-4771-9e39-72e2e33fc172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "# refs = [ # First set of references\n",
    "#           ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
    "#           # Second set of references\n",
    "#           ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.'],\n",
    "#         ]\n",
    "# sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
    "\n",
    "# bleu = BLEU()\n",
    "\n",
    "# bleu.corpus_score(sys, refs)\n",
    "# # BLEU = 48.53 82.4/50.0/45.5/37.5 (BP = 0.943 ratio = 0.944 hyp_len = 17 ref_len = 18)\n",
    "\n",
    "# bleu.get_signature()\n",
    "# # nrefs:2|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\n",
    "\n",
    "# chrf = CHRF()\n",
    "\n",
    "# chrf.corpus_score(sys, refs)\n",
    "# # chrF2 = 59.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fea000-cb5e-492b-98a5-69f44b0bf64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77574ae3-26c3-477b-b69e-4848ddc3cecf",
   "metadata": {},
   "source": [
    "## WER (Word Error Rate) and Edit Distance\n",
    "WER - https://en.m.wikipedia.org/wiki/Word_error_rate \n",
    "WER= {S+D+I}/{N}={S+D+I}/{S+D+C}\n",
    "where:\n",
    "\n",
    "**S** is the number of substitutions,\n",
    "\n",
    "**D** is the number of deletions,\n",
    "\n",
    "**I** is the number of insertions,\n",
    "\n",
    "**C** is the number of correct words,\n",
    "\n",
    "**N** is the number of words in the reference (N=S+D+C)\n",
    "\n",
    "Edit Distance (Levenshtein distance) https://en.m.wikipedia.org/wiki/Edit_distance\n",
    "Takes two either been an insert/remove/replace of characters.\n",
    "\n",
    "i.e. \n",
    "Input:   str1 = “geek”, str2 = “gesek”\n",
    "\n",
    "Output:  1\n",
    "\n",
    "Explanation: We can convert str1 into str2 by inserting a ‘s’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79f5a6-f152-4273-a616-6a264ba6ea0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515953e-e2d6-4863-8787-2954a1a78144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A WER function I found online.\n",
    "# Seems be going off legth of characters in simplification process,\n",
    "# But in a fair few of the conversions the phrases get longer.\n",
    "\n",
    "def wer(ref, hyp ,debug=True):\n",
    "    r = ref.split()\n",
    "    h = hyp.split()\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    DEL_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    SUB_PENALTY = 1\n",
    "    \n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "    \n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "    \n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        # print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    # if debug:\n",
    "    #     lines = reversed(lines)\n",
    "    #     for line in lines:\n",
    "    #         print(line)\n",
    "            \n",
    "        # Removing all the extra bits we dont need at this second\n",
    "\n",
    "        # print(\"#correct words \" + str(numCor))\n",
    "        # print(\"#substitutions \" + str(numSub))\n",
    "        # print(\"#deleltions \" + str(numDel))\n",
    "        # print(\"#insertions \" + str(numIns))\n",
    "    return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    \n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "    print({'WER':wer_result, 'numCorrect':numCor, 'numSubstitutions':numSub, 'numInserts':numIns, 'numDelete':numDel, \"numCount\": len(r)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c84366-d2b6-4560-a531-17729aa88c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing first sample\n",
    "wer(df.iloc[0][0], df.iloc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06ee29-42c1-4143-86a1-3e2d02462c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc329c-3495-4bf3-80ab-19aa5cbb8fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing Sample 2\n",
    "wer(df.iloc[2][0], df.iloc[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ee65b-ef56-4065-9b31-d3d9c96cc754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking if the length of the characters are of different lengths for the 2nd Sample.\n",
    "len(df.iloc[2][0]), len(df.iloc[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50291e0-bfa8-4e18-b7bd-0d40d10dbeec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Finding the samples with differences in characters...\")\n",
    "for i in range(150):\n",
    "    if len(df.iloc[i][0])> len(df.iloc[i][1]):\n",
    "        print(i, len(df.iloc[i][0]), len(df.iloc[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad58c4b-f32f-454e-a565-f6b36c1fe9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing on the original to simplified sentences with the WER metric\n",
    "# returing sentence as whole string so not able to give us much insight\n",
    "# Only returning if it is a WER of 0 if it is the same,\n",
    "# Or returning a WER of 1.0 if it is differnt\n",
    "\n",
    "# \n",
    "print('Testing WER on simplified sentences with smaller outputs than input...')\n",
    "print('')\n",
    "for i in range(150):    \n",
    "    if len(df.iloc[i][0])> len(df.iloc[i][1]):\n",
    "        print('Sample', i+1)\n",
    "        print('Difference in Characters = ', len(df.iloc[i][0]) - len(df.iloc[i][1]))\n",
    "        print(len(df.iloc[i][0]) - len(df.iloc[i][1]))\n",
    "        wer(df.iloc[i][0], df.iloc[i][1])\n",
    "        print('*******************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db71cf6-2cf6-4be2-8265-d6bf86dc640f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just checking on a string in Enlgish how it works...\n",
    "# i = 10\n",
    "wer('I love to dance and sing', 'I love singing and i love to do dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905171ea-2c21-47d3-a675-fa9a02145ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21f33-1b85-4072-a760-1b73bb1a7d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Making a function to iterate over the whole Data set to to see what it spits out\n",
    "\n",
    "#Creating a Function to split all characters to determine how many have been changed.\n",
    "def wer_jp(original, simplified):       \n",
    "    ori = ''\n",
    "    simpi = ''\n",
    "    wer_score = []\n",
    "    for i in original:\n",
    "        ori += i + ' '\n",
    "    for i in simplified:\n",
    "        simpi += i + ' '\n",
    "    # if round(wer(ori, simpi),3):\n",
    "    print('WER Score ', round(wer(ori, simpi),3))\n",
    "        # df.insert(2, \"WER Score\", round(wer(ori, simpi), True))\n",
    "    return round(wer(ori, simpi),3)\n",
    "    return wer_score   \n",
    "\n",
    "# for i in df.index:\n",
    "#     original_text = df.iloc[i][0] \n",
    "#     simplified_text = df.iloc[i][1]\n",
    "#     print('******************')\n",
    "#     print('Testing sample ', i+1)\n",
    "#     # print('******************')\n",
    "#     wer_jp(original_text, simplified_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cce5e-e2fd-4b18-a28a-693385b7a70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['WER_score'] = wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5e6ae-5c2f-48ac-9dfe-17da875d2117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319abf77-6b77-48c3-8c92-c41222f110b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.histplot(df['WER_score'], bins=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb5c17-5dd6-4a4b-b315-4506f117071a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wer_score(predicted, simplified, debug=True):\n",
    "    '''\n",
    "    Compares the simplified ML prediction of a given text to the pre-existing simplified\n",
    "    text given with the dataframe.\n",
    "    Using the WER (word error rate) algorithm. \n",
    "    Adds the WER score as a new column to the Dataframe\n",
    "    '''\n",
    "    r = predicted.split()\n",
    "    h = simplified.split()\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    DEL_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    SUB_PENALTY = 1\n",
    "    \n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    \n",
    "    if debug:\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    \n",
    "    return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "\n",
    "def wer_jp(original, simplified):       \n",
    "    ori = ''\n",
    "    simpi = ''\n",
    "    wer_score = []\n",
    "    for i in original:\n",
    "        ori += i + ' '\n",
    "    for i in simplified:\n",
    "        simpi += i + ' '\n",
    "    print('WER Score ', round(wer(ori, simpi),3))\n",
    "    return round(wer(ori, simpi),3)\n",
    "    return wer_score\n",
    "    \n",
    "def evaluate_wer_score(df):\n",
    "    wer_list = []\n",
    "    for i in df.index:\n",
    "        original_text = df.iloc[i][0] \n",
    "        simplified_text = df.iloc[i][1]\n",
    "        wer_list.append(wer_jp(original_text, simplified_text))\n",
    "    df['WER_score'] = wer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8073e-4b2c-4142-b48e-150b4b55bde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluating(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04513fd7-56b3-4953-8359-cc104a5e00cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e629f9bc-9185-4ef0-888b-fa5c5f51fb60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensorflow Seq2Seq\n",
    "Having a play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c407a00-7bfb-49ee-ae96-bc0608ab9113",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d8006-05d5-4952-8aa0-06ddbe3d2f51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2a441-fb03-4242-bbc2-a00b90675b22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7e3d5-c95e-4e44-9518-468d9fdecf15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # remove diacritics (accents)\n",
    "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    # remove punctuation and digits\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation + string.digits), '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def prepare_text_dataset(texts, max_vocab_size=10000, max_seq_len=100):\n",
    "    vectorizer = TextVectorization(max_tokens=max_vocab_size, output_mode='int', output_sequence_length=max_seq_len)\n",
    "    vectorizer.adapt(texts)\n",
    "    return vectorizer\n",
    "\n",
    "def prepare_seq2seq_dataset(texts, vectorizer):\n",
    "    input_texts = ['<start> ' + text for text in texts]\n",
    "    target_texts = [text + ' <end>' for text in texts]\n",
    "    input_seqs = vectorizer(np.array(input_texts))[:, :-1]\n",
    "    target_seqs = vectorizer(np.array(target_texts))[:, 1:]\n",
    "    return (input_seqs, target_seqs)\n",
    "\n",
    "\n",
    "#Preprocess text and prepare the datasetfor seq2seq transformer\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "max_vocab_size = 10000\n",
    "max_seq_len = 100\n",
    "text_vectorizer = prepare_text_dataset(df['text'], max_vocab_size, max_seq_len)\n",
    "input_seqs, target_seqs = prepare_seq2seq_dataset(df['text'], text_vectorizer)\n",
    "\n",
    "\n",
    "# Let's create the seq2seq transformer model:\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "dropout_rate = 0.1\n",
    "input_vocab_size = max_vocab_size + 2\n",
    "target_vocab_size = max_vocab_size + 2\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "targets = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "encoder = keras.layers.Encoder(input_vocab_size, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
    "decoder = keras.layers.Decoder(target_vocab_size, embedding_dim, num_heads, ff_dim, dropout_rate)\n",
    "outputs = decoder(encoder(inputs), targets)\n",
    "\n",
    "model = keras.Model([inputs, targets], outputs)\n",
    "\n",
    "\n",
    "# Let's compile the model and train it on the dataset:\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit([input_seqs, target_seqs], target_seqs, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "def predict_simplified_text(model, text, vectorizer, max_len=100):\n",
    "    text = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734d768-fbf4-4ea8-b773-62adbe41f244",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  BLEU Score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example reference and candidate sentences\n",
    "reference = ['私は犬が好きです', 'あなたは猫が好きですか']\n",
    "candidate = '私は猫が好きです'\n",
    "\n",
    "# Compute the BLEU score for the candidate sentence\n",
    "# score = sentence_bleu(reference, candidate)\n",
    "# print('BLEU score:', score)\n",
    "\n",
    "score = sentence_bleu(df.iloc[0][0], df.iloc[0][1])\n",
    "print('BLEU score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846d821-3f73-425a-b749-906bc82717a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprossesssing\n",
    "import MeCab\n",
    "import sudachipy\n",
    "from pyknp import Juman\n",
    "import re\n",
    "import fasttext\n",
    "\n",
    "# Set up MeCab tokenizer\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "# Set up SudachiPy tokenizer\n",
    "sudachi = sudachipy.Dictionary().create()\n",
    "\n",
    "# Set up Juman tokenizer\n",
    "juman = Juman()\n",
    "\n",
    "# Set up FastText model\n",
    "model = fasttext.load_model('cc.ja.300.bin')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenize the text using MeCab\n",
    "    tokens = mecab.parse(text).split()\n",
    "    \n",
    "    # Tokenize the text using SudachiPy\n",
    "    sudachi_tokens = sudachi.tokenize(text)\n",
    "    sudachi_tokens = [morph.surface() for morph in sudachi_tokens]\n",
    "    \n",
    "    # Tokenize the text using Juman\n",
    "    juman_tokens = juman.analysis(text)\n",
    "    juman_tokens = [mrph.midasi for mrph in juman_tokens.mrph_list()]\n",
    "    \n",
    "    # Combine the tokens from all tokenizers\n",
    "    tokens += sudachi_tokens\n",
    "    tokens += juman_tokens\n",
    "    tokens = list(set(tokens))  # Remove duplicates\n",
    "    \n",
    "    # Remove stop words and punctuation\n",
    "    stopwords = ['は', 'を', 'に', 'が', 'の', 'て', 'と', 'た', 'し', 'する', 'れる', 'さ', 'など']\n",
    "    tokens = [token for token in tokens if token not in stopwords and not re.match(r'[。、・！？（）「」『』【】〈〉“”’‘｜\\n]+', token)]\n",
    "\n",
    "    # Insert spaces between Kanji and Hiragana/Katakana\n",
    "    text = re.sub(r'(?<=[一-龯])(?=[ぁ-んァ-ン])', ' ', text)\n",
    "    \n",
    "    # Normalize the text\n",
    "    text = sudachi.normalize(text)\n",
    "    \n",
    "    # Encode the words using FastText embedding\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]\n",
    "    \n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45276c95-37e0-4ac0-88b2-3cfb7d173584",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Look into this:\n",
    "https://huggingface.co/sonoisa/t5-base-japanese\n",
    "\n",
    "Moana is attmepting to use the same model too.. See what we can both come up with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79ffb6-924e-4a34-b9a3-879a949b991d",
   "metadata": {},
   "source": [
    "# Trying out this model ... lots of big imports to install taking a long time...\n",
    "\n",
    "https://tsmatz.wordpress.com/2022/11/25/huggingface-japanese-summarization/\n",
    "\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n",
    "\n",
    "https://github.com/huggingface/transformers/issues/4092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f28487-fad7-4ac0-9e75-eeadf4e24f53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 12:01:54.880159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 12:01:55.421668: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-22 12:01:55.421689: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-22 12:01:57.341850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 12:01:57.341969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-22 12:01:57.341984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/samuelhenderson/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# tokenize text into ids\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msonoisa/t5-base-japanese\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msonoisa/t5-base-japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:695\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1804\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1958\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:133\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extra_tokens \u001b[38;5;241m!=\u001b[39m extra_ids:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth extra_ids (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and additional_special_tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_special_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_save_slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# tokenize text into ids\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sonoisa/t5-base-japanese\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sonoisa/t5-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194b6eb-9c8a-42f1-a7a9-36aaa897dff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# sequence to id\n",
    "seq2seq = pipeline(\"summarization\", model=\"tsmatz/mt5-summarize-jp\")\n",
    "sample_text = \"サッカーのワールドカップカタール大会、世界ランキング24位でグループEに属する日本は、23日の1次リーグ初戦において、世界11位で過去4回の優勝を誇るドイツと対戦しました。試合は前半、ドイツの一方的なペースではじまりましたが、後半、日本の森保監督は攻撃的な選手を積極的に動員して流れを変えました。結局、日本は前半に1点を奪われましたが、途中出場の堂安律選手と浅野拓磨選手が後半にゴールを決め、2対1で逆転勝ちしました。ゲームの流れをつかんだ森保采配が功を奏しました。\"\n",
    "result = seq2seq(sample_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad74010-4fad-4097-9ee1-5d9191e69a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and data collator\n",
    "import torch\n",
    "from transformers import AutoConfig, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# see https://huggingface.co/docs/transformers/main_classes/configuration\n",
    "mt5_config = AutoConfig.from_pretrained(\n",
    "  \"google/mt5-small\",\n",
    "  max_length=128,\n",
    "  length_penalty=0.6,\n",
    "  no_repeat_ngram_size=2,\n",
    "  num_beams=15,\n",
    ")\n",
    "model = (TFAutoModelForSeq2SeqLM\n",
    "         .from_pretrained(\"google/mt5-small\", config=mt5_config))\n",
    "         # .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2448e75-2495-4d63-859c-02887916e60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535fad2-e3fe-4cba-a4cd-61f70c35984b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_sample_data(data):\n",
    "  # Max token size is 14536 and 215 for inputs and labels, respectively.\n",
    "  # Here I restrict these token size.\n",
    "  input_feature = t5_tokenizer(data[\"text\"], truncation=True, max_length=1024)\n",
    "  label = t5_tokenizer(data[\"summary\"], truncation=True, max_length=128)\n",
    "  return {\n",
    "    \"input_ids\": input_feature[\"input_ids\"],\n",
    "    \"attention_mask\": input_feature[\"attention_mask\"],\n",
    "    \"labels\": label[\"input_ids\"],\n",
    "  }\n",
    "\n",
    "tokenized_df = df.map(\n",
    "  tokenize_sample_data,\n",
    "  remove_columns=[\"id\", \"url\", \"title\", \"summary\", \"text\"],\n",
    "  batched=True,\n",
    "  batch_size=128)\n",
    "\n",
    "tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324c381-e22c-4258-8325-018cdd1411c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "  t5_tokenizer,\n",
    "  model=model,\n",
    "  return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9fd60-216e-4853-a42b-5e78adca32d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# define function for custom tokenization\n",
    "def tokenize_sentence(arg):\n",
    "  encoded_arg = t5_tokenizer(arg)\n",
    "  return t5_tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "# define function to get ROUGE scores with custom tokenization\n",
    "def metrics_func(eval_arg):\n",
    "  preds, labels = eval_arg\n",
    "  # Replace -100\n",
    "  labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "  # Convert id tokens to text\n",
    "  text_preds = t5_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "  text_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "  # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "  text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "  text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "  sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "  text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "  text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "  # compute ROUGE score with custom tokenization\n",
    "  return rouge_metric.compute(\n",
    "    predictions=text_preds,\n",
    "    references=text_labels,\n",
    "    tokenizer=tokenize_sentence\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282c9dd-8fcd-4c35-964e-b7056afc24ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_dataloader = DataLoader(\n",
    "  tokenized_df[\"test\"].with_format(\"torch\"),\n",
    "  collate_fn=data_collator,\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"].to(device),\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  break\n",
    "\n",
    "metrics_func([preds, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f437d3f-c6f8-4bbb-87c3-291c2349174b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = \"mt5-summarize-ja\",\n",
    "  log_level = \"error\",\n",
    "  num_train_epochs = 10,\n",
    "  learning_rate = 5e-4,\n",
    "  lr_scheduler_type = \"linear\",\n",
    "  warmup_steps = 90,\n",
    "  optim = \"adafactor\",\n",
    "  weight_decay = 0.01,\n",
    "  per_device_train_batch_size = 2,\n",
    "  per_device_eval_batch_size = 1,\n",
    "  gradient_accumulation_steps = 16,\n",
    "  evaluation_strategy = \"steps\",\n",
    "  eval_steps = 100,\n",
    "  predict_with_generate=True,\n",
    "  generation_max_length = 128,\n",
    "  save_steps = 500,\n",
    "  logging_steps = 10,\n",
    "  push_to_hub = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea0e81-a0c1-44b8-973d-b02833bb5c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer class that fine tunes the model\n",
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  data_collator = data_collator,\n",
    "  compute_metrics = metrics_func,\n",
    "  train_dataset = tokenized_ds[\"train\"],\n",
    "  eval_dataset = tokenized_ds[\"validation\"].select(range(20)),\n",
    "  tokenizer = t5_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
