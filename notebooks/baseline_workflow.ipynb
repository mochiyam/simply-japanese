{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04eecb0-8de6-4d5c-9e0a-de059928df2e",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327925a-fb15-457b-91b7-b9d283bcce2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Information on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bf8aa-48f5-4c32-ab86-7cacc1cea3a9",
   "metadata": {},
   "source": [
    "Data Fields for SNOW T15 and SNOW T23 ⛄<br>\n",
    "Resource: https://huggingface.co/datasets/snow_simplified_japanese_corpus <br>\n",
    "Paper: https://aclanthology.org/L18-1072.pdf\n",
    "\n",
    "- <strong>ID</strong>: sentence ID.\n",
    "- <strong>original_ja</strong>: original Japanese sentebolnce.\n",
    "- <strong>simplified_ja</strong>: simplified Japanese sentence.\n",
    "- <strong>original_en</strong>: original English sentence.\n",
    "- <strong>proper_noun</strong>: (included ONLY in SNOW T23) Proper nowus that the workers has extracted as proper nouns. The authors instructed workers not to rewrite proper nouns, leaving the determination of proper nouns to the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65e0c0-f8b3-4be5-8012-11c91ba07221",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef06295-331e-4e2e-84ee-538e9aead275",
   "metadata": {},
   "source": [
    "In the SNOW T15 dataset it states: <br>\n",
    "<i>Core vocabulary is restricted to 2,000 words where it is selected by accounting for several factors such as meaning preservation, variation, simplicity and the UniDic word segmentation criterion/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae1fe8-1d47-48ab-ac7f-70e4c363431e",
   "metadata": {},
   "source": [
    "#### Step 1: Take a sample size from the SNOW T15 dataset and extracted 2,000 simplified terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edd34b4-c125-445c-ace0-a1b1c130f513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (1.0.6)\n",
      "Requirement already satisfied: unidic-lite in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (1.0.8)\n",
      "Requirement already satisfied: neologdn in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: openpyxl in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: et-xmlfile in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: japanize_matplotlib in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: matplotlib in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from japanize_matplotlib) (3.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.23.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from matplotlib->japanize_matplotlib) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/.pyenv/versions/3.8.12/envs/simply-japanese/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->japanize_matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Required installations \"\"\"\n",
    "\n",
    "!pip install mecab-python3\n",
    "#These wheels include a copy of the MeCab library, but not a dictionary. \n",
    "#In order to use MeCab you'll need to install a dictionary. unidic-lite is a good one to start with:\n",
    "!pip install unidic-lite\n",
    "\n",
    "# normalization tool\n",
    "!pip install neologdn\n",
    "\n",
    "!pip install openpyxl\n",
    "\n",
    "# To be able to see in Japanese!\n",
    "!pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f78cb4-2298-4c4b-9bcb-cd7ea9ac4218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "import MeCab\n",
    "import neologdn\n",
    "import collections\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import collections\n",
    "import logging\n",
    "import time\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a2576f-7fac-48bc-94cd-c8edfbb1a1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    \"\"\"\n",
    "    Gets csv data under 'simply-japanese/data/'\n",
    "    Returns as Dataframe where columns=['original','simplified']\n",
    "    \"\"\"\n",
    "\n",
    "    # FIXME:  Make sure to\n",
    "    # 1. Change these when you transfer to .py file\n",
    "    # 2. Put these global variables somewhere else\n",
    "    \n",
    "    CURRENT_PATH = 'notebooks/Untitled.ipynb'\n",
    "    DATA_PATH = 'data/2_RawData'\n",
    "    csv_path = os.path.abspath(__file__)[:-len(CURRENT_PATH)]  + DATA_PATH\n",
    "    df = pd.read_excel(os.path.join(csv_path, file))\n",
    "    \n",
    "    df.drop(columns=['#英語(原文)','#固有名詞'], inplace=True, errors='ignore')\n",
    "    df.rename(columns={\"#日本語(原文)\": \"original\", \"#やさしい日本語\": \"simplified\"}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e798266-15fb-4aaa-a9fd-e348d22802d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FIXME: Set df in __init__ \n",
    "def term_frequency(df, col='original'):\n",
    "    \"\"\"\n",
    "    Count number of terms in a corpus\n",
    "    Ignore independent words  [\"助動詞\", \"助詞\", \"補助記号\"] and words in japanese stopwords\n",
    "    Returns collection of term and its frequency\n",
    "    \"\"\"\n",
    "    # FIXME : Need to find a way to implement japanese_stopword.txt when this file is used externally\n",
    "    jp_stopwords = stopwords.words('japanese')\n",
    "    all_terms = collections.Counter()\n",
    "    t = MeCab.Tagger(\"-O wakati\")\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row[col]\n",
    "        node = t.parseToNode(text).next\n",
    "        while node.next:\n",
    "            part_of_speech = node.feature.split(',')[0]\n",
    "            # TBD\n",
    "            if part_of_speech in [\"助動詞\", \"助詞\", \"補助記号\"] or node.surface in jp_stopwords:\n",
    "                node = node.next\n",
    "                continue\n",
    "            all_terms[node.surface] += 1\n",
    "            node = node.next\n",
    "    return all_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1317e0-a444-48ee-96a5-c6c5394cb5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_simplified_terms(df, n_most_common):\n",
    "    \"\"\"\n",
    "    Only returns simplified terms that exists in the simplified column\n",
    "    Return list until the top 'n' elements from most common\n",
    "    \"\"\"\n",
    "    # Filter out corpuses if original and simplified are exactly the same\n",
    "    diff_corpus_df = df[df['original'] != df['simplified']]\n",
    "    \n",
    "    # Create collections of original and simplified terms\n",
    "    original_terms = term_frequency(diff_corpus_df, 'original')\n",
    "    simplified_terms = term_frequency(diff_corpus_df, 'simplified')\n",
    "    \n",
    "    # Compare two collections using subtract\n",
    "    diff_terms = simplified_terms\n",
    "    diff_terms.subtract(original_terms)\n",
    "    \n",
    "    diff_terms_df = pd.DataFrame(dict(diff_terms).items(), columns=['word', 'count'])\n",
    "    return diff_terms_df[diff_terms_df['count'] >= 0].sort_values(by='count', ascending=False)['word'].tolist()[:n_most_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc32146-ec4f-4b67-be21-7c13056de149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data('SNOW_T15_10000.xlsx')\n",
    "len(get_simplified_terms(df, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e885583-5695-47c7-b61b-a646f88542b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7f57e2f-4d1f-4046-b262-2070dcfff634",
   "metadata": {},
   "source": [
    "#### Step 2: Using the 2000 list of simplified terms from Step 1, find the nearest term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1de6d-b5d1-4c51-ab1d-c8523a09271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tf_list = 2000 simplified term frequency retrieved from data\n",
    "pos_list = specified list of POS (Parts-Of-Speech)\n",
    "\n",
    "1. Go through each row in the original data\n",
    "2. Check it word is in the pos_list\n",
    "3. Check if a word is in the tf_list\n",
    "    if yes, continue to the next word\n",
    "4. If 2. is no: check the similarity of the word with all the tf_list\n",
    "5. Replace the word with maximum value and if the maximium exceeds a specified threshold\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "PSEUDO CODE\n",
    "\n",
    "threshold = minimum similarity\n",
    "for sentence in data:\n",
    "    for word in sentence:\n",
    "        if word.pos in pos_list:\n",
    "            if word in tf_list:\n",
    "                continue\n",
    "            else:\n",
    "                for tf in tf_list:\n",
    "                    list = []\n",
    "                    list.append(wv.similarity(word, tf))\n",
    "                replace word with max(list) if max(list) > threshold\n",
    "        else: continue\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1480f8-65dd-4ae6-ad4e-709b0f74a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_terms(data, term_list=term_list, wv=wv):\n",
    "    \"\"\"\n",
    "    1. Identify every POS in a sentence and if it should be replaced\n",
    "    2. Use the pre-trained Word2Vec model to get a term from term_list with closest distance to POS\n",
    "    3. Replace POS in sentence\n",
    "    4. Add new sentence to dataframe in column \"prediction\"\n",
    "    \n",
    "    input:\n",
    "    data, np.series\n",
    "    term_list, list of simplified terms\n",
    "    wv, word2vec model.wv\n",
    "    \n",
    "    output:\n",
    "    prediction, np.series\n",
    "    \"\"\"\n",
    "    logging.root.setLevel(logging.INFO)\n",
    "    \n",
    "    start = time.time()\n",
    "    # Make sure the data is a series, not a df or list\n",
    "    try:\n",
    "        assert type(data) == pd.core.series.Series\n",
    "        logging.info(\"Data file type OK\")\n",
    "    except:\n",
    "        print(\"Data file type is NOT a pd.series\")\n",
    "    \n",
    "    pos_list = (\"名詞\", \"動詞\", \"代名詞\") # POS (part of speech) that will possibly be removed\n",
    "    threshold = 0.5 # Threshold of similarity, over which a term will be replaced\n",
    "    t = MeCab.Tagger()\n",
    "    numbers_dict = {\n",
    "        \"0\":\"零\",\n",
    "        \"1\":\"一\",\n",
    "        \"2\":\"二\",\n",
    "        \"3\":\"三\",\n",
    "        \"4\":\"四\",\n",
    "        \"5\":\"五\",\n",
    "        \"6\":\"六\",\n",
    "        \"7\":\"七\",\n",
    "        \"8\":\"八\",\n",
    "        \"9\":\"九\"\n",
    "    }\n",
    "    counter = collections.Counter()\n",
    "    prediction = data.copy()\n",
    "    assert len(prediction) == len(data)     # Make sure prediction and data have the same size\n",
    "    \n",
    "    # Iterate over every sentence in the dataset\n",
    "    for idx, row in data.items():\n",
    "        row = neologdn.normalize(row)\n",
    "        logging.debug(f\"Currrent sentence: {row}\")\n",
    "        numerals = sum(c.isdigit() for c in row)\n",
    "        if numerals > 0:\n",
    "            for i in range(numerals):\n",
    "                for entry in numbers_dict:\n",
    "                    row = row.replace(entry, numbers_dict[entry])\n",
    "        sentence = []\n",
    "        # Iterate over every word in the sentence\n",
    "        node = t.parseToNode(row).next\n",
    "        while node.next:\n",
    "            word = node.feature.split(',')[8]\n",
    "            part_of_speech = node.feature.split(',')[0]\n",
    "            # If POS is not noun, pronoun or verb: add word to list and continue\n",
    "            if part_of_speech not in pos_list:\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                # If the term is already in the term list: do not replace, add word to list and continue\n",
    "                if word in term_list:\n",
    "                    sentence.append(word)\n",
    "                else:\n",
    "                    # Replace word with closest word from term list\n",
    "                    try:\n",
    "                        if wv.most_similar(word)[0][1] > threshold:\n",
    "                            closest_word = wv.most_similar(word)[0][0]\n",
    "                            sentence.append(closest_word)\n",
    "                        else:\n",
    "                            sentence.append(word)\n",
    "                    except KeyError as e:\n",
    "                        sentence.append(word)\n",
    "                        logging.warning(f\"{e}. Term will not be replaced.\")\n",
    "            counter[node.surface] += 1\n",
    "            node = node.next\n",
    "        logging.debug(sentence)\n",
    "        prediction[idx] = \"\".join(sentence)\n",
    "    \n",
    "    assert len(data) == len(prediction)  # Make sure prediction and data have the same size\n",
    "    end = time.time()\n",
    "    logging.info(end-start)\n",
    "    return prediction\n",
    "            \n",
    "            \n",
    "predictions = replace_terms(X150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee45dff-f5a8-4e6e-862c-a285e6b17324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32725547-8e1c-420d-912d-c3ed6d2832ea",
   "metadata": {},
   "source": [
    "# 3. Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84ff93-4aaa-4e8a-942f-0561e074125b",
   "metadata": {},
   "source": [
    "### 3.1) WER SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc16002-d961-47ba-9d07-645fc49f6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_score(predicted, simplified, debug=True):\n",
    "    '''\n",
    "    Compares the simplified ML prediction of a given text to the pre-existing simplified\n",
    "    text given with the dataframe.\n",
    "    Using the WER (word error rate) algorithm.\n",
    "    Adds the WER score as a new column to the Dataframe\n",
    "    '''\n",
    "    r = predicted.split()\n",
    "    h = simplified.split()\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "    DEL_PENALTY = 1\n",
    "    INS_PENALTY = 1\n",
    "    SUB_PENALTY = 1\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    return (numSub + numDel + numIns) / (float) (len(r))\n",
    "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "    \n",
    "    \n",
    "def wer_jp(original, simplified):\n",
    "    ori = ''\n",
    "    simpi = ''\n",
    "    wer_score = []\n",
    "    for i in original:\n",
    "        ori += i + ' '\n",
    "    for i in simplified:\n",
    "        simpi += i + ' '\n",
    "    print('WER Score ', round(wer(ori, simpi),3))\n",
    "    return round(wer(ori, simpi),3)\n",
    "    return wer_score\n",
    "\n",
    "\n",
    "def evaluate_wer_score(df):\n",
    "    wer_list = []\n",
    "    for i in df.index:\n",
    "        original_text = df.iloc[i][0]\n",
    "        simplified_text = df.iloc[i][1]\n",
    "        wer_list.append(wer_jp(original_text, simplified_text))\n",
    "    df['WER_score'] = wer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f291a-69ca-4a79-99b2-03e18f7ded63",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6fd1b-fb33-408a-969b-ec5fff3d87c5",
   "metadata": {},
   "source": [
    "# 3.1) Data Organization and Clean Up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8eacf-b13a-4b4f-8a89-ac83ce9db324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imported libraries go here for Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15aed2-a563-4d94-89b3-97a81772597c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb2145ed41b1af7fe232120fa962063b18150619d590444428e8892237256527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
